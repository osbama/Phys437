{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQMfbwn5T_Ju"
   },
   "outputs": [],
   "source": [
    "!pip install pennylane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjwybQXT_Jv"
   },
   "source": [
    "# Symmetry-invariant quantum machine learning force fields\n",
    "\n",
    "Symmetries are ubiquitous in physics. From condensed matter to particle\n",
    "physics, they have helped us make connections and formulate new\n",
    "theories. In the context of machine learning, inductive bias has proven\n",
    "to be successful in the presence of symmetries. This framework, known as\n",
    "geometric deep learning, often enjoys better generalization and\n",
    "trainability. In this demo, we will learn how to use geometric quantum\n",
    "machine learning to drive molecular dynamics as introduced in recent\n",
    "research. We will take as an example a triatomic molecule of $H_2O.$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "First, let's introduce the overall playground of this work: **molecular\n",
    "dynamics (MD)**. MD is an essential computational simulation method to\n",
    "analyze the dynamics of atoms or molecules in a chemical system. The\n",
    "simulations can be used to obtain macroscopic thermodynamic properties\n",
    "of ergodic systems. Within the simulation, Newton\\'s equations of motion\n",
    "are numerically integrated. Therefore, it is crucial to have access to\n",
    "the forces acting on the constituents of the system or, equivalently,\n",
    "the potential energy surface (PES), from which we can obtain the atomic\n",
    "forces. Previous research by presented variational quantum learning\n",
    "models (VQLMs) that were able to learn the potential energy and atomic\n",
    "forces of a selection of molecules from *ab initio* reference data.\n",
    "\n",
    "The description of molecules can be greatly simplified by considering\n",
    "inherent **symmetries**. For example, actions such as translation,\n",
    "rotation, or the interchange of identical atoms or molecules leave the\n",
    "system unchanged. To achieve better performance, it is thus desirable to\n",
    "include this information in our model. To do so, the data input can\n",
    "simply be made invariant itself, e.g., by making use of so-called\n",
    "symmetry functions--hence yielding invariant energy predictions.\n",
    "\n",
    "In this demo, we instead take the high road and design an intrinsically\n",
    "symmetry-aware model based on equivariant quantum neural networks.\n",
    "Equivariant machine learning models have demonstrated many advantages\n",
    "such as being more robust to noisy data and enjoying better\n",
    "generalization capabilities. Moreover, this has the additional advantage\n",
    "of relaxing the need for data preprocessing, as the raw Cartesian\n",
    "coordinates can be given directly as inputs to the learning model.\n",
    "\n",
    "An overview of the workflow is shown in the figure below. First, the\n",
    "relevant symmetries are identified and used to build the quantum machine\n",
    "model. We then train it on the PES of some molecule, e.g. $H_2O,$ and\n",
    "finally obtain the forces by computing the gradient of the learned PES.\n",
    "\n",
    "![](Hands_on_8_images/overview.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to incorporate symmetries into machine learning models, we need\n",
    "a few concepts from group theory. A formal course on the subject is out\n",
    "of the scope of the present document, which is why we have the next sections on\n",
    "equivariant graph\n",
    "embedding \n",
    "and geometric quantum machine\n",
    "learning . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Geometric Quantum Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "Symmetries are at the heart of physics. Indeed in condensed matter and\n",
    "particle physics we often define a thing simply by the symmetries it\n",
    "adheres to. What does symmetry mean for those in machine learning? In\n",
    "this context the ambition is straightforward --- it is a means to reduce\n",
    "the parameter space and improve the trained model\\'s ability to\n",
    "sucessfully label unseen data, i.e., its ability to generalise.\n",
    "\n",
    "Suppose we have a learning task and the data we are learning from has an\n",
    "underlying symmetry. For example, consider a game of Noughts and Crosses\n",
    "(aka Tic-tac-toe): if we win a game, we would have won it if the board\n",
    "was rotated or flipped along any of the lines of symmetry. Now if we\n",
    "want to train an algorithm to spot the outcome of these games, we can\n",
    "either ignore the existence of this symmetry or we can somehow include\n",
    "it. The advantage of paying attention to the symmetry is it identifies\n",
    "multiple configurations of the board as \\'the same thing\\' as far as the\n",
    "symmetry is concerned. This means we can reduce our parameter space, and\n",
    "so the amount of data our algorithm must sift through is immediately\n",
    "reduced. Along the way, the fact that our learning model must encode a\n",
    "symmetry that actually exists in the system we are trying to represent\n",
    "naturally encourages our results to be more generalisable. The encoding\n",
    "of symmetries into our learning models is where the term *equivariance*\n",
    "will appear. We will see that demanding that certain symmetries are\n",
    "included in our models means that the mappings that make up our\n",
    "algorithms must be such that we could transform our input data with\n",
    "respect to a certain symmetry, then apply our mappings, and this would\n",
    "be the same as applying the mappings and then transforming the output\n",
    "data with the same symmetry. This is the technical property that gives\n",
    "us the name \\\"equavariant learning\\\".\n",
    "\n",
    "In classical machine learning, this area is often referred to as\n",
    "geometric deep learning (GDL) due to the traditional association of\n",
    "symmetry to the world of geometry, and the fact that these\n",
    "considerations usually focus on deep neural networks (see or for a broad\n",
    "introduction). We will refer to the quantum computing version of this as\n",
    "*quantum geometric machine learning* (QGML).\n",
    "\n",
    "# Representation theory in circuits\n",
    "\n",
    "\n",
    "The first thing to discuss is how do we work with symmetries in the\n",
    "first place? The answer lies in the world of group representation\n",
    "theory.\n",
    "\n",
    "First, let\\'s define what we mean by a group:\n",
    "\n",
    "**Definition**: A group is a set $G$ together with a binary operation on\n",
    "$G$, here denoted $\\circ,$ that combines any two elements $a$ and $b$ to\n",
    "form an element of $G,$ denoted $a \\circ b,$ such that the following\n",
    "three requirements, known as group axioms, are satisfied as follows:\n",
    "\n",
    "1.  **Associativity**: For all $a, b, c$ in $G,$ one has\n",
    "    $(a \\circ b) \\circ c=a \\circ (b \\circ c).$\n",
    "\n",
    "2.  \n",
    "\n",
    "    **Identity element**: There exists an element $e$ in $G$ such that, for every $a$ in $G,$ one\n",
    "\n",
    "    :   has $e \\circ a=a$ and $a \\circ e=a.$ Such an element is unique.\n",
    "        It is called the identity element of the group.\n",
    "\n",
    "3.  \n",
    "\n",
    "    **Inverse element**: For each $a$ in $G,$ there exists an element $b$ in $G$\n",
    "\n",
    "    :   such that $a \\circ b=e$ and $b \\circ a=e,$ where $e$ is the\n",
    "        identity element. For each $a,$ the element $b$ is unique: it is\n",
    "        called the inverse of $a$ and is commonly denoted $a^{-1}.$\n",
    "\n",
    "With groups defined, we are in a position to articulate what a\n",
    "representation is: Let $\\varphi$ be a map sending $g$ in group $G$ to a\n",
    "linear map $\\varphi(g): V \\rightarrow V,$ for some vector space $V,$\n",
    "which satisfies\n",
    "\n",
    "$$\\varphi\\left(g_{1} g_{2}\\right)=\\varphi\\left(g_{1}\\right) \\circ \\varphi\\left(g_{2}\\right) \\quad \\text { for all } g_{1}, g_{2} \\in G.$$\n",
    "\n",
    "The idea here is that just as elements in a group act on each other to\n",
    "reach further elements, i.e., $g\\circ h = k,$ a representation sends us\n",
    "to a mapping acting on a vector space such that\n",
    "$\\varphi(g)\\circ \\varphi(h) = \\varphi(k).$ In this way we are\n",
    "representing the structure of the group as a linear map. For a\n",
    "representation, our mapping must send us to the general linear group\n",
    "$GL(n)$ (the space of invertible $n \\times n$ matrices with matrix\n",
    "multiplication as the group multiplication). Note how this is both a\n",
    "group, and by virtue of being a collection of invertible matrices, also\n",
    "a set of linear maps (they\\'re all invertble matrices that can act on\n",
    "row vectors). Fundamentally, representation theory is based on the\n",
    "prosaic observation that linear algebra is easy and group theory is\n",
    "abstract. So what if we can study groups via linear maps?\n",
    "\n",
    "Now due to the importance of unitarity in quantum mechnics, we are\n",
    "particularly interested in the unitary representations: representations\n",
    "where the linear maps are unitary matrices. If we can identify these\n",
    "then we will have a way to naturally encode groups in quantum circuits\n",
    "(which are mostly made up of unitary gates).\n",
    "\n",
    "![](Hands_on_8_images/sphere_equivariant.png)\n",
    "\n",
    "How does all this relate to symmetries? Well, a large class of\n",
    "symmetries can be characterised as a group, where all the elements of\n",
    "the group leave some space we are considering unchanged. Let\\'s consider\n",
    "an example: the symmetries of a sphere. Now when we think of this\n",
    "symmetry we probably think something along the lines of \\\"it\\'s the same\n",
    "no matter how we rotate it, or flip it left to right, etc\\\". There is\n",
    "this idea of being invariant under some operation. We also have the idea\n",
    "of being able to undo these actions: if we rotate one way, we can rotate\n",
    "it back. If we flip the sphere right-to-left we can flip it\n",
    "left-to-right to get back to where we started (notice too all these\n",
    "inverses are unique). Trivially we can also do nothing. What exactly are\n",
    "we describing here? We have elements that correspond to an action on a\n",
    "sphere that can be inverted and for which there exists an identity. It\n",
    "is also trivially the case here that if we consider three operations a,\n",
    "b, c from the set of rotations and reflections of the sphere, that if we\n",
    "combine two of them together then\n",
    "$a\\circ (b \\circ c) = (a\\circ b) \\circ c.$ The operations are\n",
    "associative. These features turn out to literally define a group!\n",
    "\n",
    "As we\\'ve seen the group in itself is a very abstract creature; this is\n",
    "why we look to its representations. The group labels what symmetries we\n",
    "care about, they tell us the mappings that our system is invariant\n",
    "under, and the unitary representations show us how those symmetries look\n",
    "on a particular space of unitary matrices. If we want to encode the\n",
    "structure of the symmeteries in a quantum circuit we must restrict our\n",
    "gates to being unitary representations of the group.\n",
    "\n",
    "There remains one question: *what is equivariance?* With our newfound\n",
    "knowledge of group representation theory we are ready to tackle this.\n",
    "Let $G$ be our group, and $V$ and $W,$ with elements $v$ and $w$\n",
    "respectively, be vector spaces over some field $F$ with a map $f$\n",
    "between them. Suppose we have representations\n",
    "$\\varphi: G \\rightarrow GL(V)$ and $\\psi: G \\rightarrow GL(W).$\n",
    "Furthermore, let\\'s write $\\varphi_g$ for the representation of $g$ as a\n",
    "linear map on $V$ and $\\psi_g$ as the same group element represented as\n",
    "a linear map on $W$ respectively. We call $f$ *equivariant* if\n",
    "\n",
    "$$f(\\varphi_g(v))=\\psi_g(f(v)) \\quad \\text { for all } g\\in G.$$\n",
    "\n",
    "The importance of such a map in machine learning is that if, for\n",
    "example, our neural network layers are equivariant maps then two inputs\n",
    "that are related by some intrinsic symmetry (maybe they are reflections)\n",
    "preserve this information in the outputs.\n",
    "\n",
    "Consider the following figure for example. What we see is a board with a\n",
    "cross in a certain square on the left and some numerical encoding of\n",
    "this on the right, where the 1 is where the X is in the number grid. We\n",
    "present an equivariant mapping between these two spaces with respect to\n",
    "a group action that is a rotation or a swap (here a $\\pi$ rotation). We\n",
    "can either apply a group action to the original grid and then map to the\n",
    "number grid, or we could map to the number grid and then apply the group\n",
    "action. Equivariance demands that the result of either of these\n",
    "procedures should be the same.\n",
    "\n",
    "![](Hands_on_8_images/equivariant-example.jpg)\n",
    "\n",
    "Given the vast amount of input data required to train a neural network\n",
    "the principle that one can pre-encode known symmetry structures into the\n",
    "network allows us to learn better and faster. Indeed it is the reason\n",
    "for the success of convolutional neural networks (CNNs) for image\n",
    "analysis, where it is known they are equivariant with respect to\n",
    "translations. They naturally encode the idea that a picture of a dog is\n",
    "symmetrically related to the same picture slid to the left by n pixels,\n",
    "and they do this by having neural network layers that are equivariant\n",
    "maps. With our focus on unitary representations (and so quantum\n",
    "circuits) we are looking to extend this idea to quantum machine\n",
    "learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noughts and Crosses\n",
    "\n",
    "\n",
    "Let\\'s look at the game of noughts and crosses, as inspired by. Two\n",
    "players take turns to place a O or an X, depending on which player they\n",
    "are, in a 3x3 grid. The aim is to get three of your symbols in a row,\n",
    "column, or diagonal. As this is not always possible depending on the\n",
    "choices of the players, there could be a draw. Our learning task is to\n",
    "take a set of completed games labelled with their outcomes and teach the\n",
    "algorithm to identify these correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This board of nine elements has the symmetry of the square, also known\n",
    "as the *dihedral group*. This means it is symmetric under\n",
    "$\\frac{\\pi}{2}$ rotations and flips about the lines of symmetry of a\n",
    "square (vertical, horizontal, and both diagonals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Hands_on_8_images/NandC_sym.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The question is, how do we encode this in our QML problem?**\n",
    "\n",
    "First, let us encode this problem classically. We will consider a\n",
    "nine-element vector $v,$ each element of which identifies a square of\n",
    "the board. The entries themselves can be $+1$,$0,$$-1,$ representing a\n",
    "nought, no symbol, or a cross. The label is one-hot encoded in a vector\n",
    "$y=(y_O,y_- , y_X)$ with $+1$ in the correct label and $-1$ in the\n",
    "others. For instance (-1,-1,1) would represent an X in the relevant\n",
    "position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the quantum model let us take nine qubits and let them\n",
    "represent squares of our board. We\\'ll initialise them all as\n",
    "$|0\\rangle,$ which we note leaves the board invariant under the\n",
    "symmetries of the problem (flip and rotate all you want, it\\'s still\n",
    "going to be zeroes whatever your mapping). We will then look to apply\n",
    "single qubit $R_x(\\theta)$ rotations on individual qubits, encoding each\n",
    "of the possibilities in the board squares at an angle of\n",
    "$\\frac{2\\pi}{3}$ from each other. For our parameterised gates we will\n",
    "have a single-qubit $R_x(\\theta_1)$ and $R_y(\\theta_2)$ rotation at each\n",
    "point. We will then use $CR_y(\\theta_3)$ for two-qubit entangling gates.\n",
    "This implies that, for each encoding, crudely, we\\'ll need 18\n",
    "single-qubit rotation parameters and $\\binom{9}{2}=36$ two-qubit gate\n",
    "rotations. Let\\'s see how, by using symmetries, we can reduce this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![..](Hands_on_8_images/grid.jpg)\n",
    "\n",
    "The indexing of our game board.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secret will be to encode the symmetries into the gate set so the\n",
    "observables we are interested in inherently respect the symmetries. How\n",
    "do we do this? We need to select the collections of gates that commute\n",
    "with the symmetries. In general, we can use the twirling formula for\n",
    "this:\n",
    "\n",
    "Tip:\n",
    "\n",
    "Let $\\mathcal{S}$ be the group that encodes our symmetries and $U$ be a\n",
    "unitary representation of $\\mathcal{S}.$ Then,\n",
    "\n",
    "$$\\mathcal{T}_{U}[X]=\\frac{1}{|\\mathcal{S}|} \\sum_{s \\in \\mathcal{S}} U(s) X U(s)^{\\dagger}$$\n",
    "\n",
    "defines a projector onto the set of operators commuting with all\n",
    "elements of the representation, i.e.,\n",
    "$\\left[\\mathcal{T}_{U}[X], U(s)\\right]=$ 0 for all $X$ and\n",
    "$s \\in \\mathcal{S}.$\n",
    "\n",
    "The twirling process applied to an arbitrary unitary will give us a new\n",
    "unitary that commutes with the group as we require. We remember that\n",
    "unitary gates typically have the form $W = \\exp(-i\\theta H),$ where $H$\n",
    "is a Hermitian matrix called a *generator*, and $\\theta$ may be fixed or\n",
    "left as a free parameter. A recipe for creating a unitary that commutes\n",
    "with our symmetries is to *twirl the generator of the gate*, i.e., we\n",
    "move from the gate $W = \\exp(-i\\theta H)$ to the gate\n",
    "$W' = \\exp(-i\\theta\\mathcal{T}_U[H]).$ When each term in the twirling\n",
    "formula acts on different qubits, then this unitary would further\n",
    "simplify to\n",
    "\n",
    "$$W' = \\bigotimes_{s\\in\\mathcal{S}}U(s)\\exp(-i\\tfrac{\\theta}{\\vert\\mathcal{S}\\vert})U(s)^\\dagger.$$\n",
    "\n",
    "For simplicity, we can absorb the normalization factor\n",
    "$\\vert\\mathcal{S}\\vert$ into the free parameter $\\theta.$\n",
    "\n",
    "So let\\'s look again at our choice of gates: single-qubit $R_x(\\theta)$\n",
    "and $R_y(\\theta)$ rotations, and entangling two-qubit $CR_y(\\phi)$\n",
    "gates. What will we get by twirling these?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular instance we can see the action of the twirling\n",
    "operation geometrically as the symmetries involved are all permutations.\n",
    "Let\\'s consider the $R_x$ rotation acting on one qubit. Now if this\n",
    "qubit is in the centre location on the grid, then we can flip around any\n",
    "symmetry axis we like, and this operation leaves the qubit invariant, so\n",
    "we\\'ve identified one equivariant gate immediately. If the qubit is on\n",
    "the corners, then the flipping will send this qubit rotation to each of\n",
    "the other corners. Similarly, if a qubit is on the central edge then the\n",
    "rotation gate will be sent round the other edges. So we can see that the\n",
    "twirling operation is a sum over all the possible outcomes of performing\n",
    "the symmetry action (the sum over the symmetry group actions). Having\n",
    "done this we can see that for a single-qubit rotation the invariant maps\n",
    "are rotations on the central qubit, at all the corners, and at all the\n",
    "central edges (when their rotation angles are fixed to be the same).\n",
    "\n",
    "As an example consider the following figure, where we take a $R_x$ gate\n",
    "in the corner and then apply all the symmetries of a square. The result\n",
    "of this twirling leads us to have the same gate at all the corners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Hands_on_8_images/twirl.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For entangling gates the situation is similar. There are three invariant\n",
    "classes, the centre entangled with all corners, with all edges, and the\n",
    "edges paired in a ring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of a label is obtained via a one-hot-encoding by\n",
    "measuring the expectation values of three invariant observables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$O_{-}=Z_{\\text {middle }}=Z_{4}$$\n",
    "\n",
    "$$O_{\\circ}=\\frac{1}{4} \\sum_{i \\in \\text { corners }} Z_{i}=\\frac{1}{4}\\left[Z_{0}+Z_{2}+Z_{6}+Z_{8}\\right]$$\n",
    "\n",
    "$$O_{\\times}=\\frac{1}{4} \\sum_{i \\in \\text { edges }} Z_{i}=\\frac{1}{4}\\left[Z_{1}+Z_{3}+Z_{5}+Z_{7}\\right]$$\n",
    "\n",
    "$$\\hat{\\boldsymbol{y}}=\\left(\\left\\langle O_{\\circ}\\right\\rangle,\\left\\langle O_{-}\\right\\rangle,\\left\\langle O_{\\times}\\right\\rangle\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quantum encoding of the symmetries into a learning problem.\n",
    "A prediction for a given data point will be obtained by selecting the\n",
    "class for which the observed expectation value is the largest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a specific encoding and have decided on our observables\n",
    "we need to choose a suitable cost function to optimise. We will use an\n",
    "$l_2$ loss function acting on pairs of games and labels $D={(g,y)},$\n",
    "where $D$ is our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s now implement this!\n",
    "\n",
    "First let\\'s generate some games. Here we are creating a small program\n",
    "that will play Noughts and Crosses against itself in a random fashion.\n",
    "On completion, it spits out the winner and the winning board, with\n",
    "noughts as +1, draw as 0, and crosses as -1. There are 26,830 different\n",
    "possible games but we will only sample a few hundred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Fix seeds for reproducability\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(16)\n",
    "random.seed(16)\n",
    "\n",
    "\n",
    "#  create an empty board\n",
    "def create_board():\n",
    "    return torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "\n",
    "\n",
    "# Check for empty places on board\n",
    "def possibilities(board):\n",
    "    l = []\n",
    "    for i in range(len(board)):\n",
    "        for j in range(3):\n",
    "            if board[i, j] == 0:\n",
    "                l.append((i, j))\n",
    "    return l\n",
    "\n",
    "\n",
    "# Select a random place for the player\n",
    "def random_place(board, player):\n",
    "    selection = possibilities(board)\n",
    "    current_loc = random.choice(selection)\n",
    "    board[current_loc] = player\n",
    "    return board\n",
    "\n",
    "\n",
    "# Check if there is a winner by having 3 in a row\n",
    "def row_win(board, player):\n",
    "    for x in range(3):\n",
    "        lista = []\n",
    "        win = True\n",
    "\n",
    "        for y in range(3):\n",
    "            lista.append(board[x, y])\n",
    "\n",
    "            if board[x, y] != player:\n",
    "                win = False\n",
    "\n",
    "        if win:\n",
    "            break\n",
    "\n",
    "    return win\n",
    "\n",
    "\n",
    "# Check if there is a winner by having 3 in a column\n",
    "def col_win(board, player):\n",
    "    for x in range(3):\n",
    "        win = True\n",
    "\n",
    "        for y in range(3):\n",
    "            if board[y, x] != player:\n",
    "                win = False\n",
    "\n",
    "        if win:\n",
    "            break\n",
    "\n",
    "    return win\n",
    "\n",
    "\n",
    "# Check if there is a winner by having 3 along a diagonal\n",
    "def diag_win(board, player):\n",
    "    win1 = True\n",
    "    win2 = True\n",
    "    for x, y in [(0, 0), (1, 1), (2, 2)]:\n",
    "        if board[x, y] != player:\n",
    "            win1 = False\n",
    "\n",
    "    for x, y in [(0, 2), (1, 1), (2, 0)]:\n",
    "        if board[x, y] != player:\n",
    "            win2 = False\n",
    "\n",
    "    return win1 or win2\n",
    "\n",
    "\n",
    "# Check if the win conditions have been met or if a draw has occurred\n",
    "def evaluate_game(board):\n",
    "    winner = None\n",
    "    for player in [1, -1]:\n",
    "        if row_win(board, player) or col_win(board, player) or diag_win(board, player):\n",
    "            winner = player\n",
    "\n",
    "    if torch.all(board != 0) and winner == None:\n",
    "        winner = 0\n",
    "\n",
    "    return winner\n",
    "\n",
    "\n",
    "# Main function to start the game\n",
    "def play_game():\n",
    "    board, winner, counter = create_board(), None, 1\n",
    "    while winner == None:\n",
    "        for player in [1, -1]:\n",
    "            board = random_place(board, player)\n",
    "            counter += 1\n",
    "            winner = evaluate_game(board)\n",
    "            if winner != None:\n",
    "                break\n",
    "\n",
    "    return [board.flatten(), winner]\n",
    "\n",
    "\n",
    "def create_dataset(size_for_each_winner):\n",
    "    game_d = {-1: [], 0: [], 1: []}\n",
    "\n",
    "    while min([len(v) for k, v in game_d.items()]) < size_for_each_winner:\n",
    "        board, winner = play_game()\n",
    "        if len(game_d[winner]) < size_for_each_winner:\n",
    "            game_d[winner].append(board)\n",
    "\n",
    "    res = []\n",
    "    for winner, boards in game_d.items():\n",
    "        res += [(board, winner) for board in boards]\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "NUM_TRAINING = 450\n",
    "NUM_VALIDATION = 600\n",
    "\n",
    "# Create datasets but with even numbers of each outcome\n",
    "with torch.no_grad():\n",
    "    dataset = create_dataset(NUM_TRAINING // 3)\n",
    "    dataset_val = create_dataset(NUM_VALIDATION // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let\\'s create the relevant circuit expectation values that respect\n",
    "the symmetry classes we defined over the single-site and two-site\n",
    "measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up a nine-qubit system\n",
    "dev = qml.device(\"default.qubit\", wires=9)\n",
    "\n",
    "ob_center = qml.PauliZ(4)\n",
    "ob_corner = (qml.PauliZ(0) + qml.PauliZ(2) + qml.PauliZ(6) + qml.PauliZ(8)) * (1 / 4)\n",
    "ob_edge = (qml.PauliZ(1) + qml.PauliZ(3) + qml.PauliZ(5) + qml.PauliZ(7)) * (1 / 4)\n",
    "\n",
    "\n",
    "# Now let's encode the data in the following qubit models, first with symmetry\n",
    "@qml.qnode(dev)\n",
    "def circuit(x, p):\n",
    "\n",
    "    qml.RX(x[0], wires=0)\n",
    "    qml.RX(x[1], wires=1)\n",
    "    qml.RX(x[2], wires=2)\n",
    "    qml.RX(x[3], wires=3)\n",
    "    qml.RX(x[4], wires=4)\n",
    "    qml.RX(x[5], wires=5)\n",
    "    qml.RX(x[6], wires=6)\n",
    "    qml.RX(x[7], wires=7)\n",
    "    qml.RX(x[8], wires=8)\n",
    "\n",
    "    # Centre single-qubit rotation\n",
    "    qml.RX(p[0], wires=4)\n",
    "    qml.RY(p[1], wires=4)\n",
    "\n",
    "    # Corner single-qubit rotation\n",
    "    qml.RX(p[2], wires=0)\n",
    "    qml.RX(p[2], wires=2)\n",
    "    qml.RX(p[2], wires=6)\n",
    "    qml.RX(p[2], wires=8)\n",
    "\n",
    "    qml.RY(p[3], wires=0)\n",
    "    qml.RY(p[3], wires=2)\n",
    "    qml.RY(p[3], wires=6)\n",
    "    qml.RY(p[3], wires=8)\n",
    "\n",
    "    # Edge single-qubit rotation\n",
    "    qml.RX(p[4], wires=1)\n",
    "    qml.RX(p[4], wires=3)\n",
    "    qml.RX(p[4], wires=5)\n",
    "    qml.RX(p[4], wires=7)\n",
    "\n",
    "    qml.RY(p[5], wires=1)\n",
    "    qml.RY(p[5], wires=3)\n",
    "    qml.RY(p[5], wires=5)\n",
    "    qml.RY(p[5], wires=7)\n",
    "\n",
    "    # Entagling two-qubit gates\n",
    "    # circling the edge of the board\n",
    "    qml.CRY(p[6], wires=[0, 1])\n",
    "    qml.CRY(p[6], wires=[2, 1])\n",
    "    qml.CRY(p[6], wires=[2, 5])\n",
    "    qml.CRY(p[6], wires=[8, 5])\n",
    "    qml.CRY(p[6], wires=[8, 7])\n",
    "    qml.CRY(p[6], wires=[6, 7])\n",
    "    qml.CRY(p[6], wires=[6, 3])\n",
    "    qml.CRY(p[6], wires=[0, 3])\n",
    "\n",
    "    # To the corners from the centre\n",
    "    qml.CRY(p[7], wires=[4, 0])\n",
    "    qml.CRY(p[7], wires=[4, 2])\n",
    "    qml.CRY(p[7], wires=[4, 6])\n",
    "    qml.CRY(p[7], wires=[4, 8])\n",
    "\n",
    "    # To the centre from the edges\n",
    "    qml.CRY(p[8], wires=[1, 4])\n",
    "    qml.CRY(p[8], wires=[3, 4])\n",
    "    qml.CRY(p[8], wires=[5, 4])\n",
    "    qml.CRY(p[8], wires=[7, 4])\n",
    "\n",
    "    return [qml.expval(ob_center), qml.expval(ob_corner), qml.expval(ob_edge)]\n",
    "\n",
    "\n",
    "fig, ax = qml.draw_mpl(circuit)([0] * 9, 18 * [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s also look at the same series of gates but this time they are\n",
    "applied independently from one another, so we won\\'t be preserving the\n",
    "symmetries with our gate operations. Practically this also means more\n",
    "parameters, as previously groups of gates were updated together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit_no_sym(x, p):\n",
    "\n",
    "    qml.RX(x[0], wires=0)\n",
    "    qml.RX(x[1], wires=1)\n",
    "    qml.RX(x[2], wires=2)\n",
    "    qml.RX(x[3], wires=3)\n",
    "    qml.RX(x[4], wires=4)\n",
    "    qml.RX(x[5], wires=5)\n",
    "    qml.RX(x[6], wires=6)\n",
    "    qml.RX(x[7], wires=7)\n",
    "    qml.RX(x[8], wires=8)\n",
    "\n",
    "    # Centre single-qubit rotation\n",
    "    qml.RX(p[0], wires=4)\n",
    "    qml.RY(p[1], wires=4)\n",
    "\n",
    "    # Note in this circuit the parameters aren't all the same.\n",
    "    # Previously they were identical to ensure they were applied\n",
    "    # as one combined gate. The fact they can all vary independently\n",
    "    # here means we aren't respecting the symmetry.\n",
    "\n",
    "    # Corner single-qubit rotation\n",
    "    qml.RX(p[2], wires=0)\n",
    "    qml.RX(p[3], wires=2)\n",
    "    qml.RX(p[4], wires=6)\n",
    "    qml.RX(p[5], wires=8)\n",
    "\n",
    "    qml.RY(p[6], wires=0)\n",
    "    qml.RY(p[7], wires=2)\n",
    "    qml.RY(p[8], wires=6)\n",
    "    qml.RY(p[9], wires=8)\n",
    "\n",
    "    # Edge single-qubit rotation\n",
    "    qml.RX(p[10], wires=1)\n",
    "    qml.RX(p[11], wires=3)\n",
    "    qml.RX(p[12], wires=5)\n",
    "    qml.RX(p[13], wires=7)\n",
    "\n",
    "    qml.RY(p[14], wires=1)\n",
    "    qml.RY(p[15], wires=3)\n",
    "    qml.RY(p[16], wires=5)\n",
    "    qml.RY(p[17], wires=7)\n",
    "\n",
    "    # Entagling two-qubit gates\n",
    "    # circling the edge of the board\n",
    "    qml.CRY(p[18], wires=[0, 1])\n",
    "    qml.CRY(p[19], wires=[2, 1])\n",
    "    qml.CRY(p[20], wires=[2, 5])\n",
    "    qml.CRY(p[21], wires=[8, 5])\n",
    "    qml.CRY(p[22], wires=[8, 7])\n",
    "    qml.CRY(p[23], wires=[6, 7])\n",
    "    qml.CRY(p[24], wires=[6, 3])\n",
    "    qml.CRY(p[25], wires=[0, 3])\n",
    "\n",
    "    # To the corners from the centre\n",
    "    qml.CRY(p[26], wires=[4, 0])\n",
    "    qml.CRY(p[27], wires=[4, 2])\n",
    "    qml.CRY(p[28], wires=[4, 6])\n",
    "    qml.CRY(p[29], wires=[4, 8])\n",
    "\n",
    "    # To the centre from the edges\n",
    "    qml.CRY(p[30], wires=[1, 4])\n",
    "    qml.CRY(p[31], wires=[3, 4])\n",
    "    qml.CRY(p[32], wires=[5, 4])\n",
    "    qml.CRY(p[33], wires=[7, 4])\n",
    "\n",
    "    return [qml.expval(ob_center), qml.expval(ob_corner), qml.expval(ob_edge)]\n",
    "\n",
    "\n",
    "fig, ax = qml.draw_mpl(circuit_no_sym)([0] * 9, [0] * 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note again how, though these circuits have a similar form to before,\n",
    "they are parameterised differently. We need to feed the vector\n",
    "$\\boldsymbol{y}$ made up of the expectation value of these three\n",
    "operators into the loss function and use this to update our parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def encode_game(game):\n",
    "    board, res = game\n",
    "    x = board * (2 * math.pi) / 3\n",
    "    if res == 1:\n",
    "        y = [-1, -1, 1]\n",
    "    elif res == -1:\n",
    "        y = [1, -1, -1]\n",
    "    else:\n",
    "        y = [-1, 1, -1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the loss function we\\'re interested in is\n",
    "$\\mathcal{L}(\\mathcal{D})=\\frac{1}{|\\mathcal{D}|} \\sum_{(\\boldsymbol{g}, \\boldsymbol{y}) \\in \\mathcal{D}}\\|\\hat{\\boldsymbol{y}}(\\boldsymbol{g})-\\boldsymbol{y}\\|_{2}^{2}.$\n",
    "We need to define this and then we can begin our optimisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# calculate the mean square error for this classification problem\n",
    "def cost_function(params, input, target):\n",
    "    output = torch.stack([torch.hstack(circuit(x, params)) for x in input])\n",
    "    vec = output - target\n",
    "    sum_sqr = torch.sum(vec * vec, dim=1)\n",
    "    return torch.mean(sum_sqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s now train our symmetry-preserving circuit on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "params = 0.01 * torch.randn(9)\n",
    "params.requires_grad = True\n",
    "opt = optim.Adam([params], lr=1e-2)\n",
    "\n",
    "\n",
    "max_epoch = 15\n",
    "max_step = 30\n",
    "batch_size = 10\n",
    "\n",
    "encoded_dataset = list(zip(*[encode_game(game) for game in dataset]))\n",
    "encoded_dataset_val = list(zip(*[encode_game(game) for game in dataset_val]))\n",
    "\n",
    "\n",
    "def accuracy(p, x_val, y_val):\n",
    "    with torch.no_grad():\n",
    "        y_val = torch.tensor(y_val)\n",
    "        y_out = torch.stack([torch.hstack(circuit(x, p)) for x in x_val])\n",
    "        acc = torch.sum(torch.argmax(y_out, axis=1) == torch.argmax(y_val, axis=1))\n",
    "        return acc / len(x_val)\n",
    "\n",
    "\n",
    "print(f\"accuracy without training = {accuracy(params, *encoded_dataset_val)}\")\n",
    "\n",
    "x_dataset = torch.stack(encoded_dataset[0])\n",
    "y_dataset = torch.tensor(encoded_dataset[1], requires_grad=False)\n",
    "\n",
    "saved_costs_sym = []\n",
    "saved_accs_sym = []\n",
    "for epoch in range(max_epoch):\n",
    "    rand_idx = torch.randperm(len(x_dataset))\n",
    "    # Shuffled dataset\n",
    "    x_dataset = x_dataset[rand_idx]\n",
    "    y_dataset = y_dataset[rand_idx]\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for step in range(max_step):\n",
    "        x_batch = x_dataset[step * batch_size : (step + 1) * batch_size]\n",
    "        y_batch = y_dataset[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "        def opt_func():\n",
    "            opt.zero_grad()\n",
    "            loss = cost_function(params, x_batch, y_batch)\n",
    "            costs.append(loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(opt_func)\n",
    "\n",
    "    cost = np.mean(costs)\n",
    "    saved_costs_sym.append(cost)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # Compute validation accuracy\n",
    "        acc_val = accuracy(params, *encoded_dataset_val)\n",
    "        saved_accs_sym.append(acc_val)\n",
    "\n",
    "        res = [epoch + 1, cost, acc_val]\n",
    "        print(\"Epoch: {:2d} | Loss: {:3f} | Validation accuracy: {:3f}\".format(*res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the non-symmetry preserving circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params = 0.01 * torch.randn(34)\n",
    "params.requires_grad = True\n",
    "opt = optim.Adam([params], lr=1e-2)\n",
    "\n",
    "# calculate mean square error for this classification problem\n",
    "\n",
    "\n",
    "def cost_function_no_sym(params, input, target):\n",
    "    output = torch.stack([torch.hstack(circuit_no_sym(x, params)) for x in input])\n",
    "    vec = output - target\n",
    "    sum_sqr = torch.sum(vec * vec, dim=1)\n",
    "    return torch.mean(sum_sqr)\n",
    "\n",
    "\n",
    "max_epoch = 15\n",
    "max_step = 30\n",
    "batch_size = 15\n",
    "\n",
    "encoded_dataset = list(zip(*[encode_game(game) for game in dataset]))\n",
    "encoded_dataset_val = list(zip(*[encode_game(game) for game in dataset_val]))\n",
    "\n",
    "\n",
    "def accuracy_no_sym(p, x_val, y_val):\n",
    "    with torch.no_grad():\n",
    "        y_val = torch.tensor(y_val)\n",
    "        y_out = torch.stack([torch.hstack(circuit_no_sym(x, p)) for x in x_val])\n",
    "        acc = torch.sum(torch.argmax(y_out, axis=1) == torch.argmax(y_val, axis=1))\n",
    "        return acc / len(x_val)\n",
    "\n",
    "\n",
    "print(f\"accuracy without training = {accuracy_no_sym(params, *encoded_dataset_val)}\")\n",
    "\n",
    "\n",
    "x_dataset = torch.stack(encoded_dataset[0])\n",
    "y_dataset = torch.tensor(encoded_dataset[1], requires_grad=False)\n",
    "\n",
    "saved_costs = []\n",
    "saved_accs = []\n",
    "for epoch in range(max_epoch):\n",
    "    rand_idx = torch.randperm(len(x_dataset))\n",
    "    # Shuffled dataset\n",
    "    x_dataset = x_dataset[rand_idx]\n",
    "    y_dataset = y_dataset[rand_idx]\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for step in range(max_step):\n",
    "        x_batch = x_dataset[step * batch_size : (step + 1) * batch_size]\n",
    "        y_batch = y_dataset[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "        def opt_func():\n",
    "            opt.zero_grad()\n",
    "            loss = cost_function_no_sym(params, x_batch, y_batch)\n",
    "            costs.append(loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(opt_func)\n",
    "\n",
    "    cost = np.mean(costs)\n",
    "    saved_costs.append(costs)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # Compute validation accuracy\n",
    "        acc_val = accuracy_no_sym(params, *encoded_dataset_val)\n",
    "        saved_accs.append(acc_val)\n",
    "\n",
    "        res = [epoch + 1, cost, acc_val]\n",
    "        print(\"Epoch: {:2d} | Loss: {:3f} | Validation accuracy: {:3f}\".format(*res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let\\'s plot the results and see how the two training regimes\n",
    "differ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.title(\"Validation accuracies\")\n",
    "plt.plot(saved_accs_sym, \"b\", label=\"Symmetric\")\n",
    "plt.plot(saved_accs, \"g\", label=\"Standard\")\n",
    "\n",
    "plt.ylabel(\"Validation accuracy (%)\")\n",
    "plt.xlabel(\"Optimization steps\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see then is that by paying attention to the symmetries\n",
    "intrinsic to the learning problem and reflecting this in an equivariant\n",
    "gate set we have managed to improve our learning accuracies, while also\n",
    "using fewer parameters. While the symmetry-aware circuit clearly\n",
    "outperforms the naive one, it is notable however that the learning\n",
    "accuracies in both cases are hardly ideal given this is a solved game.\n",
    "So paying attention to symmetries definitely helps, but it also isn\\'t a\n",
    "magic bullet!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of symmetries in both quantum and classical machine learning is\n",
    "a developing field, so we can expect new results to emerge over the\n",
    "coming years. If you want to get involved, the references given below\n",
    "are a great place to start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An equivariant graph embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notorious problem when data comes in the form of graphs \\-- think of\n",
    "molecules or social media networks \\-- is that the numerical\n",
    "representation of a graph in a computer is not unique. For example, if\n",
    "we describe a graph via an [adjacency\n",
    "matrix](https://en.wikipedia.org/wiki/Adjacency_matrix) whose entries\n",
    "contain the edge weights as off-diagonals and node weights on the\n",
    "diagonal, any simultaneous permutation of rows and columns of this\n",
    "matrix refer to the same graph.\n",
    "\n",
    "![](Hands_on_8_images/adjacency-matrices.png)\n",
    "\n",
    "For example, the graph in the image above is represented by each of the\n",
    "two equivalent adjacency matrices. The top matrix can be transformed\n",
    "into the bottom matrix by swapping the first row with the third row,\n",
    "then swapping the third column with the third column, then the new first\n",
    "row with the second, and finally the first colum with the second.\n",
    "\n",
    "But the number of such permutations grows factorially with the number of\n",
    "nodes in the graph, which is even worse than an exponential growth!\n",
    "\n",
    "If we want computers to learn from graph data, we usually want our\n",
    "models to \\\"know\\\" that all these permuted adjacency matrices refer to\n",
    "the same object, so we do not waste resources on learning this property.\n",
    "In mathematical terms, this means that the model should be in- or\n",
    "equivariant (more about this distinction below) with respect to\n",
    "permutations. This is the basic motivation of [Geometric Deep\n",
    "Learning](https://geometricdeeplearning.com/), ideas of which have found\n",
    "their way into quantum machine learning.\n",
    "\n",
    "This tutorial shows how to implement an example of a trainable\n",
    "permutation equivariant graph embedding as proposed in [Skolik et al.\n",
    "(2022)](https://arxiv.org/pdf/2205.06109.pdf). The embedding maps the\n",
    "adjacency matrix of an undirected graph with edge and node weights to a\n",
    "quantum state, such that permutations of an adjacency matrix get mapped\n",
    "to the same states *if only we also permute the qubit registers in the\n",
    "same fashion*.\n",
    "\n",
    "## Permuted adjacency matrices describe the same graph\n",
    "\n",
    "\n",
    "Let us first verify that permuted adjacency matrices really describe one\n",
    "and the same graph. We also gain some useful data generation functions\n",
    "for later.\n",
    "\n",
    "First we create random adjacency matrices. The entry $a_{ij}$ of this\n",
    "matrix corresponds to the weight of the edge between nodes $i$ and $j$\n",
    "in the graph. We assume that graphs have no self-loops; instead, the\n",
    "diagonal elements of the adjacency matrix are interpreted as node\n",
    "weights (or \\\"node attributes\\\").\n",
    "\n",
    "Taking the example of a Twitter user retweet network, the nodes would be\n",
    "users, edge weights indicate how often two users retweet each other and\n",
    "node attributes could indicate the follower count of a user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(4324234)\n",
    "\n",
    "def create_data_point(n):\n",
    "    \"\"\"\n",
    "    Returns a random undirected adjacency matrix of dimension (n,n). \n",
    "    The diagonal elements are interpreted as node attributes.\n",
    "    \"\"\"\n",
    "    mat = rng.random((n, n))\n",
    "    A = (mat + np.transpose(mat))/2    \n",
    "    return np.round(A, decimals=2)\n",
    "\n",
    "A = create_data_point(3)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s also write a function to generate permuted versions of this\n",
    "adjacency matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def permute(A, permutation):\n",
    "    \"\"\"\n",
    "    Returns a copy of A with rows and columns swapped according to permutation. \n",
    "    For example, the permutation [1, 2, 0] swaps 0->1, 1->2, 2->0.\n",
    "    \"\"\"\n",
    "    \n",
    "    P = np.zeros((len(A), len(A)))\n",
    "    for i,j in enumerate(permutation):\n",
    "        P[i,j] = 1\n",
    "\n",
    "    return P @ A @ np.transpose(P)\n",
    "\n",
    "A_perm = permute(A, [1, 2, 0])\n",
    "print(A_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create [networkx]{.title-ref} graphs from both adjacency matrices\n",
    "and plot them, we see that they are identical as claimed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# interpret diagonal of matrix as node attributes\n",
    "node_labels = {n: A[n,n] for n in range(len(A))} \n",
    "np.fill_diagonal(A, np.zeros(len(A))) \n",
    "\n",
    "G1 = nx.Graph(A)\n",
    "pos1=nx.spring_layout(G1)\n",
    "nx.draw(G1, pos1, labels=node_labels, ax=ax1, node_size = 800, node_color = \"#ACE3FF\")\n",
    "edge_labels = nx.get_edge_attributes(G1,'weight')\n",
    "nx.draw_networkx_edge_labels(G1,pos1,edge_labels=edge_labels, ax=ax1)\n",
    "\n",
    "# interpret diagonal of permuted matrix as node attributes\n",
    "node_labels = {n: A_perm[n,n] for n in range(len(A_perm))}\n",
    "np.fill_diagonal(A_perm, np.zeros(len(A)))\n",
    "\n",
    "G2 = nx.Graph(A_perm)\n",
    "pos2=nx.spring_layout(G2)\n",
    "nx.draw(G2, pos2, labels=node_labels, ax=ax2, node_size = 800, node_color = \"#ACE3FF\")\n",
    "edge_labels = nx.get_edge_attributes(G2,'weight')\n",
    "nx.draw_networkx_edge_labels(G2,pos2,edge_labels=edge_labels, ax=ax2)\n",
    "\n",
    "ax1.set_xlim([1.2*x for x in ax1.get_xlim()])\n",
    "ax2.set_xlim([1.2*x for x in ax2.get_xlim()])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "The issue of non-unique numerical representations of graphs ultimately\n",
    "stems from the fact that the nodes in a graph do not have an intrinsic\n",
    "order, and by labelling them in a numerical data structure like a matrix\n",
    "we therefore impose an arbitrary order.\n",
    "\n",
    "\n",
    "## Permutation equivariant embeddings\n",
    "\n",
    "\n",
    "When we design a machine learning model that takes graph data, the first\n",
    "step is to encode the adjacency matrix into a quantum state using an\n",
    "embedding or quantum feature\n",
    "map $\\phi:$\n",
    "\n",
    "$$A \\rightarrow |\\phi(A)\\rangle .$$\n",
    "\n",
    "We may want the resulting quantum state to be the same for all adjacency\n",
    "matrices describing the same graph. In mathematical terms, this means\n",
    "that $\\phi$ is an *invariant* embedding with respect to simultaneous row\n",
    "and column permutations $\\pi(A)$ of the adjacency matrix:\n",
    "\n",
    "$$|\\phi(A) \\rangle = |\\phi(\\pi(A))\\rangle \\;\\; \\text{ for all } \\pi .$$\n",
    "\n",
    "However, invariance is often too strong a constraint. Think for example\n",
    "of an encoding that associates each node in the graph with a qubit. We\n",
    "might want permutations of the adjacency matrix to lead to the same\n",
    "state *up to an equivalent permutation of the qubits* $P_{\\pi},$ where\n",
    "\n",
    "$$P_{\\pi} |q_1,...,q_n \\rangle = |q_{\\textit{perm}_{\\pi}(1)}, ... q_{\\textit{perm}_{\\pi}(n)} \\rangle .$$\n",
    "\n",
    "The function $\\text{perm}_{\\pi}$ maps each index to the permuted index\n",
    "according to $\\pi.$\n",
    "\n",
    "Note: \n",
    "\n",
    "\n",
    "The operator $P_{\\pi}$ is implemented by PennyLane\\'s\n",
    "`~pennylane.Permute.`{.interpreted-text role=\"class\"}\n",
    "\n",
    "\n",
    "This results in an *equivariant* embedding with respect to permutations\n",
    "of the adjacency matrix:\n",
    "\n",
    "$$|\\phi(A) \\rangle = P_{\\pi}|\\phi(\\pi(A))\\rangle \\;\\; \\text{ for all } \\pi .$$\n",
    "\n",
    "This is exactly what the following quantum embedding is aiming to do!\n",
    "The mathematical details behind these concepts use group theory and are\n",
    "beautiful, but can be a bit daunting. Have a look at [this\n",
    "paper](https://arxiv.org/abs/2210.08566) if you want to learn more.\n",
    "\n",
    "## Implementation in PennyLane\n",
    "\n",
    "\n",
    "Let\\'s get our hands dirty with an example. As mentioned, we will\n",
    "implement the permutation-equivariant embedding suggested in [Skolik et\n",
    "al. (2022)](https://arxiv.org/pdf/2205.06109.pdf) which has this\n",
    "structure:\n",
    "\n",
    "![](Hands_on_8_images/circuit1.png)\n",
    "\n",
    "The image can be found in [Skolik et al.\n",
    "(2022)](https://arxiv.org/pdf/2205.06109.pdf) and shows one layer of the\n",
    "circuit. The $\\epsilon$ are our edge weights while $\\alpha$ describe the\n",
    "node weights, and the $\\beta,$ $\\gamma$ are variational parameters.\n",
    "\n",
    "In PennyLane this looks as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "def perm_equivariant_embedding(A, betas, gammas):\n",
    "    \"\"\"\n",
    "    Ansatz to embedd a graph with node and edge weights into a quantum state.\n",
    "    \n",
    "    The adjacency matrix A contains the edge weights on the off-diagonal, \n",
    "    as well as the node attributes on the diagonal.\n",
    "    \n",
    "    The embedding contains trainable weights 'betas' and 'gammas'.\n",
    "    \"\"\"\n",
    "    n_nodes = len(A)\n",
    "    n_layers = len(betas) # infer the number of layers from the parameters\n",
    "    \n",
    "    # initialise in the plus state\n",
    "    for i in range(n_nodes):\n",
    "        qml.Hadamard(i)\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i):\n",
    "            \t# factor of 2 due to definition of gate\n",
    "                qml.IsingZZ(2*gammas[l]*A[i,j], wires=[i,j]) \n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            qml.RX(A[i,i]*betas[l], wires=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this ansatz in a circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 2\n",
    "\n",
    "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def eqc(adjacency_matrix, observable, trainable_betas, trainable_gammas):\n",
    "    \"\"\"Circuit that uses the permutation equivariant embedding\"\"\"\n",
    "    \n",
    "    perm_equivariant_embedding(adjacency_matrix, trainable_betas, trainable_gammas)\n",
    "    return qml.expval(observable)\n",
    "\n",
    "\n",
    "A = create_data_point(n_qubits)\n",
    "betas = rng.random(n_layers)\n",
    "gammas = rng.random(n_layers)\n",
    "observable = qml.PauliX(0) @ qml.PauliX(1) @ qml.PauliX(3)\n",
    "\n",
    "qml.draw_mpl(eqc, decimals=2)(A, observable, betas, gammas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the equivariance\n",
    "===========================\n",
    "\n",
    "Let\\'s now check if the circuit is really equivariant!\n",
    "\n",
    "This is the expectation value we get using the original adjacency matrix\n",
    "as an input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_A = eqc(A, observable, betas, gammas)\n",
    "print(\"Model output for A:\", result_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we permute the adjacency matrix, this is what we get:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "perm = [2, 3, 0, 1, 4]\n",
    "A_perm = permute(A, perm)\n",
    "result_Aperm = eqc(A_perm, observable, betas, gammas)\n",
    "print(\"Model output for permutation of A: \", result_Aperm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the two values different? Well, we constructed an *equivariant*\n",
    "ansatz, not an *invariant* one! Remember, an *invariant* ansatz means\n",
    "that embedding a permutation of the adjacency matrix leads to the same\n",
    "state as an embedding of the original matrix. An *equivariant* ansatz\n",
    "embeds the permuted adjacency matrix into a state where the qubits are\n",
    "permuted as well.\n",
    "\n",
    "As a result, the final state before measurement is only the same if we\n",
    "permute the qubits in the same manner that we permute the input\n",
    "adjacency matrix. We could insert a permutation operator\n",
    "`qml.Permute(perm)` to achieve this, or we simply permute the wires of\n",
    "the observables!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "observable_perm = qml.PauliX(perm[0]) @ qml.PauliX(perm[1]) @ qml.PauliX(perm[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything should work out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_Aperm = eqc(A_perm, observable_perm, betas, gammas)\n",
    "print(\"Model output for permutation of A, and with permuted observable: \", result_Aperm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voil!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n",
    "Equivariant graph embeddings can be combined with other equivariant\n",
    "parts of a quantum machine learning pipeline (like measurements and the\n",
    "cost function). [Skolik et al.\n",
    "(2022)](https://arxiv.org/pdf/2205.06109.pdf), for example, use such a\n",
    "pipeline as part of a reinforcement learning scheme that finds heuristic\n",
    "solutions for the traveling salesman problem. Their simulations compare\n",
    "a fully equivariant model to circuits that break permutation\n",
    "equivariance and show that it performs better, confirming that if we\n",
    "know about structure in our data, we should try to use this knowledge in\n",
    "machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum models as Fourier series\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration is based on the paper *The effect of data encoding on\n",
    "the expressive power of variational quantum machine learning models* by\n",
    "[Schuld, Sweke, and Meyer (2020)](https://arxiv.org/abs/2008.08605).\n",
    "\n",
    "![](Hands_on_8_images/scheme_thumb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper links common quantum machine learning models designed for\n",
    "near-term quantum computers to Fourier series (and, in more general, to\n",
    "Fourier-type sums). With this link, the class of functions a quantum\n",
    "model can learn (i.e., its \\\"expressivity\\\") can be characterized by the\n",
    "model\\'s control of the Fourier series\\' frequencies and coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background\n",
    "==========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref. considers quantum machine learning models of the form\n",
    "\n",
    "$$f_{\\boldsymbol \\theta}(x) = \\langle 0| U^{\\dagger}(x,\\boldsymbol \\theta) M U(x, \\boldsymbol \\theta) | 0 \\rangle$$\n",
    "\n",
    "where $M$ is a measurement observable and $U(x, \\boldsymbol \\theta)$ is\n",
    "a variational quantum circuit that encodes a data input $x$ and depends\n",
    "on a set of parameters $\\boldsymbol \\theta.$ Here we will restrict\n",
    "ourselves to one-dimensional data inputs, but the paper motivates that\n",
    "higher-dimensional features simply generalize to multi-dimensional\n",
    "Fourier series.\n",
    "\n",
    "The circuit itself repeats $L$ layers, each consisting of a\n",
    "data-encoding circuit block $S(x)$ and a trainable circuit block\n",
    "$W(\\boldsymbol \\theta)$ that is controlled by the parameters\n",
    "$\\boldsymbol \\theta.$ The data encoding block consists of gates of the\n",
    "form $\\mathcal{G}(x) = e^{-ix H},$ where $H$ is a Hamiltonian. A\n",
    "prominent example of such gates are Pauli rotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper shows how such a quantum model can be written as a\n",
    "Fourier-type sum of the form\n",
    "\n",
    "$$f_{ \\boldsymbol \\theta}(x) = \\sum_{\\omega \\in \\Omega} c_{\\omega}( \\boldsymbol \\theta) \\; e^{i  \\omega x}.$$\n",
    "\n",
    "As illustrated in the picture below (which is Figure 1 from the paper),\n",
    "the \\\"encoding Hamiltonians\\\" in $S(x)$ determine the set $\\Omega$ of\n",
    "available \\\"frequencies\\\", and the remainder of the circuit, including\n",
    "the trainable parameters, determines the coefficients $c_{\\omega}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Hands_on_8_images/scheme.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper demonstrates many of its findings for circuits in which\n",
    "$\\mathcal{G}(x)$ is a single-qubit Pauli rotation gate. For example, it\n",
    "shows that $r$ repetitions of a Pauli rotation-encoding gate in\n",
    "\\\"sequence\\\" (on the same qubit, but with multiple layers $r=L$) or in\n",
    "\\\"parallel\\\" (on $r$ different qubits, with $L=1$) creates a quantum\n",
    "model that can be expressed as a *Fourier series* of the form\n",
    "\n",
    "$$f_{ \\boldsymbol \\theta}(x) = \\sum_{n \\in \\Omega} c_{n}(\\boldsymbol \\theta) e^{i  n x},$$\n",
    "\n",
    "where $\\Omega = \\{ -r, \\dots, -1, 0, 1, \\dots, r\\}$ is a spectrum of\n",
    "consecutive integer-valued frequencies up to degree $r.$\n",
    "\n",
    "As a result, we expect quantum models that encode an input $x$ by $r$\n",
    "Pauli rotations to only be able to fit Fourier series of at most degree\n",
    "$r.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this demonstration\n",
    "==========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments below investigate this \\\"Fourier-series\\\"-like nature of\n",
    "quantum models by showing how to reproduce the simulations underlying\n",
    "Figures 3, 4 and 5 in Section II of the paper:\n",
    "\n",
    "-   **Figures 3 and 4** are function-fitting experiments, where quantum\n",
    "    models with different encoding strategies have the task to fit\n",
    "    Fourier series up to a certain degree. As in the paper, we will use\n",
    "    examples of qubit-based quantum circuits where a single data feature\n",
    "    is encoded via Pauli rotations.\n",
    "-   **Figure 5** plots the Fourier coefficients of randomly sampled\n",
    "    instances from a family of quantum models which is defined by some\n",
    "    parametrized ansatz.\n",
    "\n",
    "The code is presented so you can easily modify it in order to play\n",
    "around with other settings and models. The settings used in the paper\n",
    "are given in the various subsections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let\\'s make some imports and define a standard loss\n",
    "function for the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I: Fitting Fourier series with serial Pauli-rotation encoding\n",
    "==================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will reproduce Figures 3 and 4 from the paper. These show how\n",
    "quantum models that use Pauli rotations as data-encoding gates can only\n",
    "fit Fourier series up to a certain degree. The degree corresponds to the\n",
    "number of times that the Pauli gate gets repeated in the quantum model.\n",
    "\n",
    "Let us consider circuits where the encoding gate gets repeated\n",
    "sequentially (as in Figure 2a of the paper). For simplicity we will only\n",
    "look at single-qubit circuits:\n",
    "\n",
    "![](Hands_on_8_images/single_qubit_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a target function\n",
    "========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a (classical) target function which will be used as a\n",
    "\\\"ground truth\\\" that the quantum model has to fit. The target function\n",
    "is constructed as a Fourier series of a specific degree.\n",
    "\n",
    "We also allow for a rescaling of the data by a hyperparameter `scaling`,\n",
    "which we will do in the quantum model as well. As shown in, for the\n",
    "quantum model to learn the classical model in the experiment below, the\n",
    "scaling of the quantum model and the target function have to match,\n",
    "which is an important observation for the design of quantum machine\n",
    "learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "degree = 1  # degree of the target function\n",
    "scaling = 1  # scaling of the data\n",
    "coeffs = [0.15 + 0.15j] * degree  # coefficients of non-zero frequencies\n",
    "coeff0 = 0.1  # coefficient of zero frequency\n",
    "\n",
    "\n",
    "def target_function(x):\n",
    "    \"\"\"Generate a truncated Fourier series, where the data gets re-scaled.\"\"\"\n",
    "    res = coeff0\n",
    "    for idx, coeff in enumerate(coeffs):\n",
    "        exponent = np.complex128(scaling * (idx + 1) * x * 1j)\n",
    "        conj_coeff = np.conjugate(coeff)\n",
    "        res += coeff * np.exp(exponent) + conj_coeff * np.exp(-exponent)\n",
    "    return np.real(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s have a look at it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 70, requires_grad=False)\n",
    "target_y = np.array([target_function(x_) for x_ in x], requires_grad=False)\n",
    "\n",
    "plt.plot(x, target_y, c=\"black\")\n",
    "plt.scatter(x, target_y, facecolor=\"white\", edgecolor=\"black\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "To reproduce the figures in the paper, you can use the following\n",
    "settings in the cells above:\n",
    "\n",
    "-   For the settings\n",
    "\n",
    "        degree = 1\n",
    "        coeffs = (0.15 + 0.15j) * degree\n",
    "        coeff0 = 0.1\n",
    "\n",
    "    this function is the ground truth\n",
    "    $g(x) = \\sum_{n=-1}^1 c_{n} e^{-nix}$ from Figure 3 in the paper.\n",
    "\n",
    "-   To get the ground truth $g'(x) = \\sum_{n=-2}^2 c_{n} e^{-nix}$ with\n",
    "    $c_0=0.1,$ $c_1 = c_2 = 0.15 - 0.15i$ from Figure 3, you need to\n",
    "    increase the degree to two:\n",
    "\n",
    "        degree = 2\n",
    "\n",
    "-   The ground truth from Figure 4 can be reproduced by changing the\n",
    "    settings to:\n",
    "\n",
    "        degree = 5\n",
    "        coeffs = (0.05 + 0.05j) * degree\n",
    "        coeff0 = 0.0\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the serial quantum model\n",
    "===============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the quantum model itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaling = 1\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data-encoding circuit block.\"\"\"\n",
    "    qml.RX(scaling * x, wires=0)\n",
    "\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    qml.Rot(theta[0], theta[1], theta[2], wires=0)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def serial_quantum_model(weights, x):\n",
    "\n",
    "    for theta in weights[:-1]:\n",
    "        W(theta)\n",
    "        S(x)\n",
    "\n",
    "    # (L+1)'th unitary\n",
    "    W(weights[-1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following cell multiple times, each time sampling\n",
    "different weights, and therefore different quantum models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "r = 1  # number of times the encoding gets repeated (here equal to the number of layers)\n",
    "weights = (\n",
    "    2 * np.pi * np.random.random(size=(r + 1, 3), requires_grad=True)\n",
    ")  # some random initial weights\n",
    "\n",
    "x = np.linspace(-6, 6, 70, requires_grad=False)\n",
    "random_quantum_model_y = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, random_quantum_model_y, c=\"blue\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what weights are picked, the single qubit model for\n",
    "[L=1]{.title-ref} will always be a sine function of a fixed frequency.\n",
    "The weights merely influence the amplitude, y-shift, and phase of the\n",
    "sine.\n",
    "\n",
    "This observation is formally derived in Section II.A of the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "You can increase the number of layers. Figure 4 from the paper, for\n",
    "example, uses the settings `L=1`, `L=3` and `L=5`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let\\'s look at the circuit we just created:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(qml.draw(serial_quantum_model)(weights, x[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the target\n",
    "===========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to optimize the weights in order to fit the ground\n",
    "truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "\n",
    "max_steps = 50\n",
    "opt = qml.AdamOptimizer(0.3)\n",
    "batch_size = 25\n",
    "cst = [cost(weights, x, target_y)]  # initial cost\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Select batch of data\n",
    "    batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "    x_batch = x[batch_index]\n",
    "    y_batch = target_y[batch_index]\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "    # Save, and possibly print, the current cost\n",
    "    c = cost(weights, x, target_y)\n",
    "    cst.append(c)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\"Cost at step {0:3}: {1}\".format(step + 1, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue training, you may just run the above cell again. Once you\n",
    "are happy, you can use the trained model to predict function values, and\n",
    "compare them with the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = [serial_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, target_y, c=\"black\")\n",
    "plt.scatter(x, target_y, facecolor=\"white\", edgecolor=\"black\")\n",
    "plt.plot(x, predictions, c=\"blue\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s also have a look at the cost during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(cst)), cst)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylim(0, 0.23)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the initial settings and enough training steps, the quantum model\n",
    "learns to fit the ground truth perfectly. This is expected, since the\n",
    "number of Pauli-rotation-encoding gates and the degree of the ground\n",
    "truth Fourier series are both one.\n",
    "\n",
    "If the ground truth\\'s degree is larger than the number of layers in the\n",
    "quantum model, the fit will look much less accurate. And finally, we\n",
    "also need to have the correct scaling of the data: if one of the models\n",
    "changes the `scaling` parameter (which effectively scales the\n",
    "frequencies), fitting does not work even with enough encoding\n",
    "repetitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "You will find that the training takes much longer, and needs a lot more\n",
    "steps to converge for larger L. Some initial weights may not even\n",
    "converge to a good solution at all; the training seems to get stuck in a\n",
    "minimum.\n",
    "\n",
    "It is an open research question whether for asymptotically large L, the\n",
    "single qubit model can fit *any* function by constructing arbitrary\n",
    "Fourier coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II: Fitting Fourier series with parallel Pauli-rotation encoding\n",
    "=====================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to repeat the function-fitting experiment for a circuit\n",
    "where the Pauli rotation gate gets repeated $r$ times on *different*\n",
    "qubits, using a single layer $L=1.$\n",
    "\n",
    "As shown in the paper, we expect similar results to the serial model: a\n",
    "Fourier series of degree $r$ can only be fitted if there are at least\n",
    "$r$ repetitions of the encoding gate in the quantum model. However, in\n",
    "practice this experiment is a bit harder, since the dimension of the\n",
    "trainable unitaries $W$ grows quickly with the number of qubits.\n",
    "\n",
    "In the paper, the investigations are made with the assumption that the\n",
    "purple trainable blocks $W$ are arbitrary unitaries. We could use the\n",
    "`~.pennylane.templates.ArbitraryUnitary`{.interpreted-text role=\"class\"}\n",
    "template, but since this template requires a number of parameters that\n",
    "grows exponentially with the number of qubits ($4^L-1$ to be precise),\n",
    "this quickly becomes cumbersome to train.\n",
    "\n",
    "We therefore follow Figure 4 in the paper and use an ansatz for $W.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Hands_on_8_images/parallel_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parallel quantum model\n",
    "=================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ansatz is PennyLane\\'s layer structure called\n",
    "`~.pennylane.templates.StronglyEntanglingLayers`{.interpreted-text\n",
    "role=\"class\"}, and as the name suggests, it has itself a user-defined\n",
    "number of layers (which we will call \\\"ansatz layers\\\" to avoid\n",
    "confusion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pennylane.templates import StronglyEntanglingLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s have a quick look at the ansatz itself for 3 qubits by making a\n",
    "dummy circuit of 2 ansatz layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_ansatz_layers = 2\n",
    "n_qubits = 3\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def ansatz(weights):\n",
    "    StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return qml.expval(qml.Identity(wires=0))\n",
    "\n",
    "\n",
    "weights_ansatz = 2 * np.pi * np.random.random(size=(n_ansatz_layers, n_qubits, 3))\n",
    "print(qml.draw(ansatz, level=\"device\")(weights_ansatz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the actual quantum model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaling = 1\n",
    "r = 3\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=r)\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data-encoding circuit block.\"\"\"\n",
    "    for w in range(r):\n",
    "        qml.RX(scaling * x, wires=w)\n",
    "\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    StronglyEntanglingLayers(theta, wires=range(r))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def parallel_quantum_model(weights, x):\n",
    "\n",
    "    W(weights[0])\n",
    "    S(x)\n",
    "    W(weights[1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can sample random weights and plot the model function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainable_block_layers = 3\n",
    "weights = 2 * np.pi * np.random.random(size=(2, trainable_block_layers, r, 3), requires_grad=True)\n",
    "\n",
    "x = np.linspace(-6, 6, 70, requires_grad=False)\n",
    "random_quantum_model_y = [parallel_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, random_quantum_model_y, c=\"blue\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "==================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is done exactly as before, but it may take a lot\n",
    "longer this time. We set a default of 70 steps, which you should\n",
    "increase if necessary. Small models of \\<6 qubits usually converge after\n",
    "a few hundred steps at most---but this depends on your settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [parallel_quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "\n",
    "max_steps = 70\n",
    "opt = qml.AdamOptimizer(0.3)\n",
    "batch_size = 25\n",
    "cst = [cost(weights, x, target_y)]  # initial cost\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # select batch of data\n",
    "    batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "    x_batch = x[batch_index]\n",
    "    y_batch = target_y[batch_index]\n",
    "\n",
    "    # update the weights by one optimizer step\n",
    "    weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "    # save, and possibly print, the current cost\n",
    "    c = cost(weights, x, target_y)\n",
    "    cst.append(c)\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\"Cost at step {0:3}: {1}\".format(step + 1, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = [parallel_quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, target_y, c=\"black\")\n",
    "plt.scatter(x, target_y, facecolor=\"white\", edgecolor=\"black\")\n",
    "plt.plot(x, predictions, c=\"blue\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(cst)), cst)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "\n",
    "To reproduce the right column in Figure 4 from the paper, use the\n",
    "correct ground truth, $r=3$ and\n",
    "[\\`trainable\\_block\\_layers=3] as well as sufficiently\n",
    "many training steps. The amount of steps depends on the initial weights\n",
    "and other hyperparameters, and in some settings training may not\n",
    "converge to zero error at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part III: Sampling Fourier coefficients\n",
    "=======================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use a trainable ansatz above, it is possible that even with\n",
    "enough repetitions of the data-encoding Pauli rotation, the quantum\n",
    "model cannot fit the circuit, since the expressivity of quantum models\n",
    "also depends on the Fourier coefficients the model can create.\n",
    "\n",
    "Figure 5 in shows Fourier coefficients from quantum models sampled from\n",
    "a model family defined by an ansatz for the trainable circuit block. For\n",
    "this we need a function that numerically computes the Fourier\n",
    "coefficients of a periodic function f with period $2 \\pi.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fourier_coefficients(f, K):\n",
    "    \"\"\"\n",
    "    Computes the first 2*K+1 Fourier coefficients of a 2*pi periodic function.\n",
    "    \"\"\"\n",
    "    n_coeffs = 2 * K + 1\n",
    "    t = np.linspace(0, 2 * np.pi, n_coeffs, endpoint=False)\n",
    "    y = np.fft.rfft(f(t)) / t.size\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your quantum model\n",
    "=========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a quantum model. This could be any model, using a\n",
    "qubit or continuous-variable circuit, or one of the quantum models from\n",
    "above. We will use a slight derivation of the `parallel_qubit_model()`\n",
    "from above, this time using the\n",
    "`~.pennylane.templates.BasicEntanglerLayers`{.interpreted-text\n",
    "role=\"class\"} ansatz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pennylane.templates import BasicEntanglerLayers\n",
    "\n",
    "scaling = 1\n",
    "n_qubits = 4\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    \"\"\"Data encoding circuit block.\"\"\"\n",
    "    for w in range(n_qubits):\n",
    "        qml.RX(scaling * x, wires=w)\n",
    "\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    BasicEntanglerLayers(theta, wires=range(n_qubits))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_model(weights, x):\n",
    "\n",
    "    W(weights[0])\n",
    "    S(x)\n",
    "    W(weights[1])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be handy to define a function that samples different random\n",
    "weights of the correct size for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_ansatz_layers = 1\n",
    "\n",
    "\n",
    "def random_weights():\n",
    "    return 2 * np.pi * np.random.random(size=(2, n_ansatz_layers, n_qubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the first few Fourier coefficients for samples from\n",
    "this model. The samples are created by randomly sampling different\n",
    "parameters using the `random_weights()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_coeffs = 5\n",
    "n_samples = 100\n",
    "\n",
    "\n",
    "coeffs = []\n",
    "for i in range(n_samples):\n",
    "\n",
    "    weights = random_weights()\n",
    "\n",
    "    def f(x):\n",
    "        return np.array([quantum_model(weights, x_) for x_ in x])\n",
    "\n",
    "    coeffs_sample = fourier_coefficients(f, n_coeffs)\n",
    "    coeffs.append(coeffs_sample)\n",
    "\n",
    "coeffs = np.array(coeffs)\n",
    "coeffs_real = np.real(coeffs)\n",
    "coeffs_imag = np.imag(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s plot the real vs. the imaginary part of the coefficients. As a\n",
    "sanity check, the $c_0$ coefficient should be real, and therefore have\n",
    "no contribution on the y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_coeffs = len(coeffs_real[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, n_coeffs, figsize=(15, 4))\n",
    "\n",
    "for idx, ax_ in enumerate(ax):\n",
    "    ax_.set_title(r\"$c_{}$\".format(idx))\n",
    "    ax_.scatter(\n",
    "        coeffs_real[:, idx],\n",
    "        coeffs_imag[:, idx],\n",
    "        s=20,\n",
    "        facecolor=\"white\",\n",
    "        edgecolor=\"red\",\n",
    "    )\n",
    "    ax_.set_aspect(\"equal\")\n",
    "    ax_.set_ylim(-1, 1)\n",
    "    ax_.set_xlim(-1, 1)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with different quantum models, you will find that some\n",
    "quantum models create different distributions over the coefficients than\n",
    "others. For example `BasicEntanglingLayers` (with the default Pauli-X\n",
    "rotation) seems to have a structure that forces the even Fourier\n",
    "coefficients to zero, while `StronglyEntanglingLayers` will have a\n",
    "non-zero variance for all supported coefficients.\n",
    "\n",
    "Note also how the variance of the distribution decreases for growing\n",
    "orders of the coefficients---an effect linked to the convergence of a\n",
    "Fourier series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "\n",
    "\n",
    "To reproduce the results from Figure 5 you have to change the ansatz (no\n",
    "unitary, `BasicEntanglerLayers` or `StronglyEntanglingLayers`, and set\n",
    "`n_ansatz_layers` either to $1$ or $5$). The `StronglyEntanglingLayers`\n",
    "requires weights of shape `size=(2, n_ansatz_layers, n_qubits, 3)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous-variable model\n",
    "=========================\n",
    "\n",
    "Ref. mentions that a phase rotation in continuous-variable quantum\n",
    "computing has a spectrum that supports *all* Fourier frequecies. To play\n",
    "with this model, we finally show you the code for a continuous-variable\n",
    "circuit. For example, to see its Fourier coefficients run the cell\n",
    "below, and then re-run the two cells above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "var = 2\n",
    "n_ansatz_layers = 1\n",
    "dev_cv = qml.device(\"default.gaussian\", wires=1)\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    qml.Rotation(x, wires=0)\n",
    "\n",
    "\n",
    "def W(theta):\n",
    "    \"\"\"Trainable circuit block.\"\"\"\n",
    "    for r_ in range(n_ansatz_layers):\n",
    "        qml.Displacement(theta[0], theta[1], wires=0)\n",
    "        qml.Squeezing(theta[2], theta[3], wires=0)\n",
    "\n",
    "\n",
    "@qml.qnode(dev_cv)\n",
    "def quantum_model(weights, x):\n",
    "    W(weights[0])\n",
    "    S(x)\n",
    "    W(weights[1])\n",
    "    return qml.expval(qml.QuadX(wires=0))\n",
    "\n",
    "\n",
    "def random_weights():\n",
    "    return np.random.normal(size=(2, 5 * n_ansatz_layers), loc=0, scale=var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "\n",
    "\n",
    "To find out what effect so-called \\\"non-Gaussian\\\" gates like the `Kerr`\n",
    "gate have, you need to install the [strawberryfields\n",
    "plugin](https://pennylane-sf.readthedocs.io/en/latest/) and change the\n",
    "device to\n",
    "\n",
    "``` {.python}\n",
    "dev_cv = qml.device('strawberryfields.fock', wires=1, cutoff_dim=50)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjwybQXT_Jv"
   },
   "source": [
    "## Equivariant Quantum Machine learning\n",
    "\n",
    "In the following, we will denote elements of a symmetry group $G$ with\n",
    "$g \\in G.$ $G$ could be for instance the rotation group $SO(3),$ or the\n",
    "permutation group $S_n.$ Groups are often easier understood in terms of\n",
    "their representation $V_g : \\mathcal{V} \\rightarrow \\mathcal{V}$ which\n",
    "maps group elements to invertible linear operations, i.e. to $GL(n),$ on\n",
    "some vector space $\\mathcal{V}.$ We call a function\n",
    "$f: \\mathcal{V} \\rightarrow \\mathcal{W}$ *invariant* with respect to the\n",
    "action of the group, if\n",
    "\n",
    "$$f(V_g(v)) = f(v),  \\text{  for all } g \\in G.$$\n",
    "\n",
    "The concept of *equivariance* is a bit weaker, as it only requires the\n",
    "function to *commute* with the group action, instead of remaining\n",
    "constant. In mathematical terms, we require that\n",
    "\n",
    "$$f(V_g(v)) = \\mathcal{R}_g(f(v)),  \\text{  for all } g \\in G,$$\n",
    "\n",
    "with $\\mathcal{R}$ being a representation of $G$ on the vector space\n",
    "$\\mathcal{W}.$ These concepts are important in machine learning, as they\n",
    "tell us how the internal structure of the data, described by the group,\n",
    "is conserved when passing through the model. In the remaining, we will\n",
    "refer to $\\mathcal{V}$ and $V_g$ as the data space and the\n",
    "representation on it, respectively, and $\\mathcal{W}$ and\n",
    "$\\mathcal{R}_g$ as the qubit space and the symmetry action on it,\n",
    "respectively.\n",
    "\n",
    "Now that we have the basics, we will focus on the task at hand: building\n",
    "an equivariant quantum neural network for chemistry!\n",
    "\n",
    "We use a [quantum reuploading\n",
    "model](https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series/),\n",
    "which consists of a variational ansatz $M_\\Theta(\\mathcal{X})$ applied\n",
    "to some initial state $|\\psi_0\\rangle.$ Here, $\\mathcal{X}$ denotes the\n",
    "description of a molecular configuration, i.e., the set of Cartesian\n",
    "coordinates of the atoms. The quantum circuit is given by\n",
    "\n",
    "$$M_\\Theta(\\mathcal{X}) = \\left[ \\prod_{d=D}^1 \\Phi(\\mathcal{X}) \\mathcal{U}_d(\\vec{\\theta}_d) \\right] \\Phi(\\mathcal{X}),$$\n",
    "\n",
    "and depends on both data $\\mathcal{X}$ and trainable parameters\n",
    "$\\Theta = \\{\\vec{\\theta}_d\\}_{d=1}^D.$ It is built by interleaving\n",
    "parametrized trainable layers $U_d(\\vec{\\theta}_d)$ with data encoding\n",
    "layers $\\Phi(\\mathcal{X}).$ The corresponding quantum function\n",
    "$f_{\\Theta}(\\mathcal{X})$ is then given by the expectation value of a\n",
    "chosen observable $O$\n",
    "\n",
    "$$f_\\Theta(\\mathcal{X}) = \\langle \\psi_0 | M_\\Theta(\\mathcal{X})^\\dagger O M_\\Theta(\\mathcal{X}) |\\psi_0 \\rangle.$$\n",
    "\n",
    "For the cases of a diatomic molecule (e.g. $LiH$) and a triatomic\n",
    "molecule of two atom types (e.g. $H_2O$), panel (a) of the following\n",
    "figure displays the descriptions of the chemical systems by the\n",
    "Cartesian coordinates of their atoms, while the general circuit\n",
    "formulation of the corresponding symmetry-invariant VQLM for these cases\n",
    "is shown in panel (b). Note that we will only consider the triatomic\n",
    "molecule $H_2O$ in the rest of this demo.\n",
    "\n",
    "![](Hands_on_8_images/siVQLM_monomer.jpg)\n",
    "\n",
    "An overall invariant model is composed of four ingredients: an invariant\n",
    "initial state, an equivariant encoding layer, equivariant trainable\n",
    "layers, and finally an invariant observable. Here, equivariant encoding\n",
    "means that applying the symmetry transformation first on the atomic\n",
    "configuration $\\mathcal{X}$ and then encoding it into the qubits\n",
    "produces the same results as first encoding $\\mathcal{X}$ and then\n",
    "letting the symmetry act on the qubits, i.e.,\n",
    "\n",
    "$$\\Phi(V_g[\\mathcal{X}]) = \\mathcal{R}_g \\Phi(\\mathcal{X}) \\mathcal{R}_g^\\dagger,$$\n",
    "\n",
    "where $V_g$ and $\\mathcal{R}_g$ denote the symmetry representation on\n",
    "the data and qubit level, respectively.\n",
    "\n",
    "For the trainable layer, equivariance means that the order of applying\n",
    "the symmetry and the parametrized operations does not matter:\n",
    "\n",
    "$$\\left[\\mathcal{U}_d(\\vec{\\theta}_d), \\mathcal{R}_g\\right]=0.$$\n",
    "\n",
    "Furthermore, we need to find an invariant observable\n",
    "$O = \\mathcal{R}_g O \\mathcal{R}_g^\\dagger$ and an initial state\n",
    "$|\\psi_0\\rangle = \\mathcal{R}_g |\\psi_0\\rangle,$ i.e., which can absorb\n",
    "the symmetry action. Putting all this together results in a\n",
    "symmetry-invariant VQLM as required.\n",
    "\n",
    "In this demo, we will consider the example of a triatomic molecule of\n",
    "two atom types, such as a water molecule. In this case, the system is\n",
    "invariant under translations, rotations, and the exchange of the two\n",
    "hydrogen atoms. Translational symmetry is included by taking the central\n",
    "atom as the origin. Therefore, we only need to encode the coordinates of\n",
    "the two identical *active* atoms, which we will call $\\vec{x}_1$ and\n",
    "$\\vec{x}_2.$\n",
    "\n",
    "Let's implement the model depicted above!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1VgPOp-T_Jw"
   },
   "source": [
    "# Implementation of the VQLM\n",
    "\n",
    "We start by importing the libraries that we will need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwof4vRlT_Jx"
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dauxcrTZT_Jx"
   },
   "source": [
    "Let us construct Pauli matrices, which are used to build the\n",
    "Hamiltonian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3c7z724T_Jx"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [1, 0]])\n",
    "Y = np.array([[0, -1.0j], [1.0j, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "\n",
    "sigmas = jnp.array(np.array([X, Y, Z]))  # Vector of Pauli matrices\n",
    "sigmas_sigmas = jnp.array(\n",
    "    np.array(\n",
    "        [\n",
    "            np.kron(X, X),\n",
    "            np.kron(Y, Y),\n",
    "            np.kron(Z, Z),\n",
    "        ]  # Vector of tensor products of Pauli matrices\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foXZy77wT_Jx"
   },
   "source": [
    "We start by considering **rotational invariance** and building an\n",
    "initial state invariant under rotation, such as the singlet state\n",
    "$|S\\rangle = \\frac{|01|10}{\\sqrt{2}}.$ A general $2n$-invariant state\n",
    "can be obtained by taking $n$-fold tensor product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bj-yygoT_Jx"
   },
   "outputs": [],
   "source": [
    "def singlet(wires):\n",
    "    # Encode a 2-qubit rotation-invariant initial state, i.e., the singlet state.\n",
    "\n",
    "    qml.Hadamard(wires=wires[0])\n",
    "    qml.PauliZ(wires=wires[0])\n",
    "    qml.PauliX(wires=wires[1])\n",
    "    qml.CNOT(wires=wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I13hwRRBT_Jy"
   },
   "source": [
    "Next, we need a rotationally equivariant data embedding. We choose to\n",
    "encode a three-dimensional data point $\\vec{x}\\in \\mathbb{R}^3$ via\n",
    "\n",
    "$$\\Phi(\\vec{x}) = \\exp\\left( -i\\alpha_\\text{enc} [xX + yY + zZ] \\right),$$\n",
    "\n",
    "where we introduce a trainable encoding angle\n",
    "$\\alpha_\\text{enc}\\in\\mathbb{R}.$ This encoding scheme is indeed\n",
    "equivariant since embedding a rotated data point is the same as\n",
    "embedding the original data point and then letting the rotation act on\n",
    "the qubits:\n",
    "$\\Phi(r(\\psi,\\theta,\\phi)\\vec{x}) = U(\\psi,\\theta,\\phi) \\Phi(\\vec{x}) U(\\psi,\\theta,\\phi)^\\dagger.$\n",
    "For this, we have noticed that any rotation on the data level can be\n",
    "parametrized by three angles $V_g = r(\\psi,\\theta,\\phi),$ which can also\n",
    "be used to parametrize the corresponding single-qubit rotation\n",
    "$\\mathcal{R}_g = U(\\psi,\\theta,\\phi),$ implemented by the usual\n",
    "[qml.rot](https://docs.pennylane.ai/en/stable/code/api/pennylane.Rot.html)\n",
    "operation. We choose to encode each atom twice in parallel, resulting in\n",
    "higher expressivity. We can do so by simply using this encoding scheme\n",
    "twice for each active atom (the two Hydrogens in our case):\n",
    "\n",
    "$$\\Phi(\\vec{x}_1, \\vec{x}_2) = \\Phi^{(1)}(\\vec{x}_1) \\Phi^{(2)}(\\vec{x}_2) \\Phi^{(3)}(\\vec{x}_1) \\Phi^{(4)}(\\vec{x}_2).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9t3xn4HT_Jy"
   },
   "outputs": [],
   "source": [
    "def equivariant_encoding(alpha, data, wires):\n",
    "    # data (jax array): cartesian coordinates of atom i\n",
    "    # alpha (jax array): trainable scaling parameter\n",
    "\n",
    "    hamiltonian = jnp.einsum(\"i,ijk\", data, sigmas)  # Heisenberg Hamiltonian\n",
    "    U = jax.scipy.linalg.expm(-1.0j * alpha * hamiltonian / 2)\n",
    "    qml.QubitUnitary(U, wires=wires, id=\"E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt9uC_sLT_Jz"
   },
   "source": [
    "Finally, we require an equivariant trainable map and an invariant\n",
    "observable. We take the Heisenberg Hamiltonian, which is rotationally\n",
    "invariant, as an inspiration. We define a single summand of it,\n",
    "$H^{(i,j)}(J) = -J\\left( X^{(i)}X^{(j)} + Y^{(i)}Y^{(j)} + Z^{(i)}Z^{(j)} \\right),$\n",
    "as a rotationally invariant two-qubit operator and choose\n",
    "\n",
    "$$O = X^{(0)}X^{(1)} + Y^{(0)}Y^{(1)} + Z^{(0)}Z^{(1)}$$\n",
    "\n",
    "as our observable.\n",
    "\n",
    "Furthermore, we can obtain an equivariant parametrized operator by\n",
    "exponentiating this Heisenberg interaction:\n",
    "\n",
    "$$RH^{(i,j)}(J) = \\exp\\left( -iH^{(i,j)}(J) \\right),$$\n",
    "\n",
    "where $J\\in\\mathbb{R}$ is a trainable parameter. By combining this\n",
    "exponentiated operator for different pairs of qubits, we can design our\n",
    "equivariant trainable layer\n",
    "\n",
    "$$\\mathcal{U}(\\vec{j}) = RH^{(1,2)}(j_1) RH^{(3,4)}(j_2) RH^{(2,3)}(j_3)$$\n",
    "\n",
    "In the case of a triatomic molecule of two atom types, we need to modify\n",
    "the previous VQLM to additionally take into account the **invariance\n",
    "under permutations of the same atom types**.\n",
    "\n",
    "Interchanging two atoms is represented on the data level by simply\n",
    "interchanging the corresponding coordinates,\n",
    "$V_g = \\sigma(\\vec{x}_1, \\vec{x}_2) = (\\vec{x}_2, \\vec{x}_1).$ On the\n",
    "Hilbert space this is represented by swapping the corresponding qubits,\n",
    "$\\mathcal{R}_g = U(i,j) = SWAP(i,j).$\n",
    "\n",
    "The singlet state is not only rotationally invariant but also\n",
    "permutationally invariant under swapping certain qubit pairs, so we can\n",
    "keep it. The previous embedding scheme for one data point can be\n",
    "extended for embedding two atoms and we see that this is indeed not only\n",
    "rotationally equivariant but also equivariant with respect to\n",
    "permutations, since encoding two swapped atoms is just the same as\n",
    "encoding the atoms in the original order and then swapping the qubits:\n",
    "$\\Phi\\left( \\sigma(\\vec{x}_1, \\vec{x}_2) \\right) = SWAP(i,j) \\Phi(\\vec{x}_1, \\vec{x}_2) SWAP(i,j).$\n",
    "Again, we choose to encode each atom twice as depicted above.\n",
    "\n",
    "For the invariant observable $O,$ we note that our Heisenberg\n",
    "interaction is invariant under the swapping of the two involved qubits,\n",
    "therefore we can make use of the same observable as before.\n",
    "\n",
    "For the equivariant parametrized layer we need to be careful when it\n",
    "comes to the selection of qubit pairs in order to obtain equivariance,\n",
    "i.e., operations that commute with the swappings. This is fulfilled by\n",
    "coupling only the qubits with are neighbors with respect to the\n",
    "1-2-3-4-1 ring topology, leading to the following operation:\n",
    "\n",
    "$$\\mathcal{U}(\\vec{j}) = RH^{(1,2)}(j_1) RH^{(3,4)}(j_2) RH^{(2,3)}(j_3) RH^{(1,4)}(j_3)$$\n",
    "\n",
    "In code, we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDKdrmryT_Jz"
   },
   "outputs": [],
   "source": [
    "def trainable_layer(weight, wires):\n",
    "    hamiltonian = jnp.einsum(\"ijk->jk\", sigmas_sigmas)\n",
    "    U = jax.scipy.linalg.expm(-1.0j * weight * hamiltonian)\n",
    "    qml.QubitUnitary(U, wires=wires, id=\"U\")\n",
    "\n",
    "\n",
    "# Invariant observable\n",
    "Heisenberg = [\n",
    "    qml.PauliX(0) @ qml.PauliX(1),\n",
    "    qml.PauliY(0) @ qml.PauliY(1),\n",
    "    qml.PauliZ(0) @ qml.PauliZ(1),\n",
    "]\n",
    "Observable = qml.Hamiltonian(np.ones((3)), Heisenberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoI4cDQDT_Jz"
   },
   "source": [
    "It has been observed that a small amount of **symmetry-breaking** (SB)\n",
    "can improve the convergence of the VQLM. We implement it by adding a\n",
    "small rotation around the $z$-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lMmAtdYT_Jz"
   },
   "outputs": [],
   "source": [
    "def noise_layer(epsilon, wires):\n",
    "    for _, w in enumerate(wires):\n",
    "        qml.RZ(epsilon[_], wires=[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoXgQEifT_Jz"
   },
   "source": [
    "When setting up the model, the hyperparameters such as the number of\n",
    "repetitions of encoding and trainable layers have to be chosen suitably.\n",
    "In this demo, we choose six layers ($D=6$) and one repetition of\n",
    "trainable gates inside each layer ($B=1$) to reduce long runtimes. Note\n",
    "that this choice differs from the original paper, so the results therein\n",
    "will not be fully reproduced within this demo. We start by defining the\n",
    "relevant hyperparameters and the VQLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-JbDcmpT_Jz"
   },
   "outputs": [],
   "source": [
    "############ Setup ##############\n",
    "D = 6  # Depth of the model\n",
    "B = 1  # Number of repetitions inside a trainable layer\n",
    "rep = 2  # Number of repeated vertical encoding\n",
    "\n",
    "active_atoms = 2  # Number of active atoms\n",
    "# Here we only have two active atoms since we fixed the oxygen (which becomes non-active) at the origin\n",
    "num_qubits = active_atoms * rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49EzojLUT_J0"
   },
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def vqlm(data, params):\n",
    "\n",
    "    weights = params[\"params\"][\"weights\"]\n",
    "    alphas = params[\"params\"][\"alphas\"]\n",
    "    epsilon = params[\"params\"][\"epsilon\"]\n",
    "\n",
    "    # Initial state\n",
    "    for i in range(rep):\n",
    "        singlet(wires=np.arange(active_atoms * i, active_atoms * (1 + i)))\n",
    "\n",
    "    # Initial encoding\n",
    "    for i in range(num_qubits):\n",
    "        equivariant_encoding(\n",
    "            alphas[i, 0], jnp.asarray(data, dtype=complex)[i % active_atoms, ...], wires=[i]\n",
    "        )\n",
    "\n",
    "    # Reuploading model\n",
    "    for d in range(D):\n",
    "        qml.Barrier()\n",
    "\n",
    "        for b in range(B):\n",
    "            # Even layer\n",
    "            for i in range(0, num_qubits - 1, 2):\n",
    "                trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "\n",
    "            # Odd layer\n",
    "            for i in range(1, num_qubits, 2):\n",
    "                trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "\n",
    "        # Symmetry-breaking\n",
    "        if epsilon is not None:\n",
    "            noise_layer(epsilon[d, :], range(num_qubits))\n",
    "\n",
    "        # Encoding\n",
    "        for i in range(num_qubits):\n",
    "            equivariant_encoding(\n",
    "                alphas[i, d + 1],\n",
    "                jnp.asarray(data, dtype=complex)[i % active_atoms, ...],\n",
    "                wires=[i],\n",
    "            )\n",
    "\n",
    "    return qml.expval(Observable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uhu73-6WT_J0"
   },
   "source": [
    "Simulation for the water molecule\n",
    "=================================\n",
    "\n",
    "We start by downloading the\n",
    "[dataset](https://zenodo.org/records/2634098), which we have prepared\n",
    "for convenience as a Python ndarray. In the following, we will load,\n",
    "preprocess and split the data into a training and testing set, following\n",
    "standard practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4ZjJpJeT_J0"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "energy = np.load(\"eqnn_force_field_data/Energy.npy\")\n",
    "forces = np.load(\"eqnn_force_field_data/Forces.npy\")\n",
    "positions = np.load(\n",
    "    \"eqnn_force_field_data/Positions.npy\"\n",
    ")  # Cartesian coordinates shape = (nbr_sample, nbr_atoms,3)\n",
    "shape = np.shape(positions)\n",
    "\n",
    "### Scaling the energy to fit in [-1,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler((-1, 1))\n",
    "\n",
    "energy = scaler.fit_transform(energy)\n",
    "forces = forces * scaler.scale_\n",
    "\n",
    "\n",
    "# Placing the oxygen at the origin\n",
    "data = np.zeros((shape[0], 2, 3))\n",
    "data[:, 0, :] = positions[:, 1, :] - positions[:, 0, :]\n",
    "data[:, 1, :] = positions[:, 2, :] - positions[:, 0, :]\n",
    "positions = data.copy()\n",
    "\n",
    "forces = forces[:, 1:, :]  # Select only the forces on the hydrogen atoms since the oxygen is fixed\n",
    "\n",
    "\n",
    "# Splitting in train-test set\n",
    "indices_train = np.random.choice(np.arange(shape[0]), size=int(0.8 * shape[0]), replace=False)\n",
    "indices_test = np.setdiff1d(np.arange(shape[0]), indices_train)\n",
    "\n",
    "E_train, E_test = (energy[indices_train, 0], energy[indices_test, 0])\n",
    "F_train, F_test = forces[indices_train, ...], forces[indices_test, ...]\n",
    "data_train, data_test = (\n",
    "    jnp.array(positions[indices_train, ...]),\n",
    "    jnp.array(positions[indices_test, ...]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1bLOTQbT_J0"
   },
   "source": [
    "We will know define the cost function and how to train the model using\n",
    "Jax. We will use the mean-square-error loss function. To speed up the\n",
    "computation, we use the decorator `@jax.jit` to do just-in-time\n",
    "compilation for this execution. This means the first execution will\n",
    "typically take a little longer with the benefit that all following\n",
    "executions will be significantly faster, see the [Jax docs on\n",
    "jitting](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yw-yWpfDT_J0"
   },
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers\n",
    "\n",
    "# We vectorize the model over the data points\n",
    "vec_vqlm = jax.vmap(vqlm, (0, None), 0)\n",
    "\n",
    "\n",
    "# Mean-squared-error loss function\n",
    "@jax.jit\n",
    "def mse_loss(predictions, targets):\n",
    "    return jnp.mean(0.5 * (predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "# Make prediction and compute the loss\n",
    "@jax.jit\n",
    "def cost(weights, loss_data):\n",
    "    data, E_target, F_target = loss_data\n",
    "    E_pred = vec_vqlm(data, weights)\n",
    "    l = mse_loss(E_pred, E_target)\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "# Perform one training step\n",
    "@jax.jit\n",
    "def train_step(step_i, opt_state, loss_data):\n",
    "\n",
    "    net_params = get_params(opt_state)\n",
    "    loss, grads = jax.value_and_grad(cost, argnums=0)(net_params, loss_data)\n",
    "\n",
    "    return loss, opt_update(step_i, grads, opt_state)\n",
    "\n",
    "\n",
    "# Return prediction and loss at inference times, e.g. for testing\n",
    "@jax.jit\n",
    "def inference(loss_data, opt_state):\n",
    "\n",
    "    data, E_target, F_target = loss_data\n",
    "    net_params = get_params(opt_state)\n",
    "\n",
    "    E_pred = vec_vqlm(data, net_params)\n",
    "    l = mse_loss(E_pred, E_target)\n",
    "\n",
    "    return E_pred, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNvRlmlgT_J0"
   },
   "source": [
    "**Parameter initialization:**\n",
    "\n",
    "We initiliase the model at the identity by setting the initial\n",
    "parameters to 0, except the first one which is chosen uniformly. This\n",
    "ensures that the circuit is shallow at the beginning and has less chance\n",
    "of suffering from the barren plateau phenomenon. Moreover, we disable\n",
    "the symmetry-breaking strategy, as it is mainly useful for larger\n",
    "systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo2Pkq4oT_J0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.zeros((num_qubits, D, B))\n",
    "weights[0] = np.random.uniform(0, np.pi, 1)\n",
    "weights = jnp.array(weights)\n",
    "\n",
    "# Encoding weights\n",
    "alphas = jnp.array(np.ones((num_qubits, D + 1)))\n",
    "\n",
    "# Symmetry-breaking (SB)\n",
    "np.random.seed(42)\n",
    "epsilon = jnp.array(np.random.normal(0, 0.001, size=(D, num_qubits)))\n",
    "epsilon = None  # We disable SB for this specific example\n",
    "epsilon = jax.lax.stop_gradient(epsilon)  # comment if we wish to train the SB weights as well.\n",
    "\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(1e-2)\n",
    "net_params = {\"params\": {\"weights\": weights, \"alphas\": alphas, \"epsilon\": epsilon}}\n",
    "opt_state = opt_init(net_params)\n",
    "running_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91n7pkwZT_J1"
   },
   "source": [
    "We train our VQLM using stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8jV_tzeT_J1"
   },
   "outputs": [],
   "source": [
    "num_batches = 5000  # number of optimization steps\n",
    "batch_size = 256  # number of training data per batch\n",
    "\n",
    "\n",
    "for ibatch in range(num_batches):\n",
    "    # select a batch of training points\n",
    "    batch = np.random.choice(np.arange(np.shape(data_train)[0]), batch_size, replace=False)\n",
    "\n",
    "    # preparing the data\n",
    "    loss_data = data_train[batch, ...], E_train[batch, ...], F_train[batch, ...]\n",
    "    loss_data_test = data_test, E_test, F_test\n",
    "\n",
    "    # perform one training step\n",
    "    loss, opt_state = train_step(num_batches, opt_state, loss_data)\n",
    "\n",
    "    # computing the test loss and energy predictions\n",
    "    E_pred, test_loss = inference(loss_data_test, opt_state)\n",
    "    running_loss.append([float(loss), float(test_loss)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Y8hJh0T_J1"
   },
   "source": [
    "Let us inspect the results. The following figure displays the training\n",
    "(in red) and testing (in blue) loss during the optimization. We observe\n",
    "that they are on top of each other, meaning that the model is training\n",
    "and generalising properly to the unseen test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-nmG3T8T_J1"
   },
   "outputs": [],
   "source": [
    "history_loss = np.array(running_loss)\n",
    "\n",
    "fontsize = 12\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(history_loss[:, 0], \"r-\", label=\"training error\")\n",
    "plt.plot(history_loss[:, 1], \"b-\", label=\"testing error\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Optimization Steps\", fontsize=fontsize)\n",
    "plt.ylabel(\"Mean Squared Error\", fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mil1IYafT_J1"
   },
   "source": [
    "## Energy predictions\n",
    "\n",
    "\n",
    "We first inspect the quality of the energy predictions. The exact test\n",
    "energy points are shown in black, while the predictions are in red. On\n",
    "the left, we see the exact data against the predicted ones (so the red\n",
    "points should be in the diagonal line), while the right plots show the\n",
    "energy as a scatter plot. The model is able to make fair predictions,\n",
    "especially near the equilibrium position. However, a few points in the\n",
    "higher energy range could be improved, e.g. by using a deeper model as\n",
    "in the original paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAYfVu-QT_J1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.title(\"Energy predictions\", fontsize=fontsize)\n",
    "plt.plot(energy[indices_test], E_pred, \"ro\", label=\"Test predictions\")\n",
    "plt.plot(energy[indices_test], energy[indices_test], \"k.-\", lw=1, label=\"Exact\")\n",
    "plt.xlabel(\"Exact energy\", fontsize=fontsize)\n",
    "plt.ylabel(\"Predicted energy\", fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Ur5B18T_J1"
   },
   "source": [
    "## Force predictions\n",
    "\n",
    "As stated at the beginning, we are interested in obtaining the forces to\n",
    "drive MD simulations. Since we have access to the potential energy\n",
    "surface, the forces are directly available by taking the gradient\n",
    "\n",
    "$$F_{i,j} = -\\nabla_{\\mathcal{X}_{ij}} E(\\mathcal{X}, \\Theta),$$\n",
    "\n",
    "where $\\mathcal{X}_{ij}$ contains the $j$ coordinate of the $i$-th atom,\n",
    "and $\\Theta$ are the trainable parameters. In our framework, we can\n",
    "simply do the following. We note that we do not require the mixed terms\n",
    "of the Jacobian, which is why we select the diagonal part using\n",
    "`numpy.einsum`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiffqsfoT_J1"
   },
   "outputs": [],
   "source": [
    "opt_params = get_params(opt_state)  # Obtain the optimal parameters\n",
    "gradient_coordinates = jax.jacobian(\n",
    "    vec_vqlm, argnums=0\n",
    ")  # Compute the gradient with respect to the Cartesian coordinates\n",
    "\n",
    "pred_forces = gradient_coordinates(jnp.array(positions.real), opt_params)\n",
    "pred_forces = -np.einsum(\n",
    "    \"iijk->ijk\", np.array(pred_forces)\n",
    ")  # We are only interested in the diagonal part of the Jacobian\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "\n",
    "fig.suptitle(\"Force predictions\", fontsize=fontsize)\n",
    "for k in range(2):\n",
    "    for l in range(3):\n",
    "\n",
    "        axs[k, l].plot(forces[indices_test, k, l], forces[indices_test, k, l], \"k.-\", lw=1)\n",
    "        axs[k, l].plot(forces[indices_test, k, l], pred_forces[indices_test, k, l], \"r.\")\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Hydrogen 1\")\n",
    "axs[1, 0].set_ylabel(\"Hydrogen 2\")\n",
    "for _, a in enumerate([\"x\", \"y\", \"z\"]):\n",
    "    axs[1, _].set_xlabel(\"{}-axis\".format(a))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tlzq3j0uT_J2"
   },
   "source": [
    "In this series of plots, we can see the predicted forces on the two\n",
    "Hydrogen atoms in the three $x,$ $y$ and $z$ directions. Again, the\n",
    "model does a fairly good job. The few points which are not on the\n",
    "diagonal can be improved using some tricks, such as incorporating the\n",
    "forces in the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEBk76vaT_J2"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In this demo, we saw how to implement a symmetry-invariant VQLM to learn\n",
    "the energy and forces of small chemical systems and trained it for the\n",
    "specific example of water. The strong points with respect to\n",
    "symmetry-agnostic techniques are better generalization, more accurate\n",
    "force predictions, resilience to small data corruption, and reduction in\n",
    "classical pre- and postprocessing, as supported by the original paper.\n",
    "\n",
    "Further work could be devoted to studying larger systems by adopting a\n",
    "more systematic fragmentation as discussed in the original paper. As an\n",
    "alternative to building symmetry-invariant quantum architectures, the\n",
    "symmetries could instead be incorporated into the training routine, such\n",
    "as recently proposed by. Finally, symmetry-aware models could be used to\n",
    "design quantum symmetry functions, which in turn could serve as\n",
    "symmetry-invariant descriptors of the chemical systems within classical\n",
    "deep learning architectures, which can be easily operated and trained at\n",
    "scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLyIeFA5T_J2"
   },
   "source": [
    "## References\n",
    "\n",
    "1. Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Velikovi (2021). Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv:2104.13478\n",
    "2. Quynh T. Nguyen, Louis Schatzki, Paolo Braccia, Michael Ragone, Patrick J. Coles, Frdric Sauvage, Martn Larocca, and M. Cerezo (2022). Theory for Equivariant Quantum Neural Networks. arXiv:2210.08566\n",
    "3. Andrea Skolik, Michele Cattelan, Sheir Yarkoni,Thomas Baeck and Vedran Dunjko (2022). Equivariant quantum circuits for learning on weighted graphs. arXiv:2205.06109\n",
    "4. Quynh T. Nguyen, Louis Schatzki, Paolo Braccia, Michael Ragone, Patrick J. Coles, Frdric Sauvage, Martn Larocca and Marco Cerezo (2022). Theory for Equivariant Quantum Neural Networks. arXiv:2210.08566\n",
    "5. Johannes Jakob Meyer, Marian Mularski, Elies Gil-Fuster, Antonio Anna Mele, Francesco Arzani, Alissa Wilms, Jens Eisert (2022). Exploiting symmetry in variational quantum machine learning. arXiv:2205.06217\n",
    "6. Andrea Skolik, Michele Cattelan, Sheir Yarkoni,Thomas Baeck and Vedran Dunjko (2022). Equivariant quantum circuits for learning on    weighted graphs.[arXiv:2205.06109](https://arxiv.org/abs/2205.06109)\n",
    "7.  Quynh T. Nguyen, Louis Schatzki, Paolo Braccia, Michael Ragone, Patrick J. Coles, Frdric Sauvage, Martn Larocca and Marco Cerezo     (2022). Theory for Equivariant Quantum Neural Networks. [arXiv:2210.08566](https://arxiv.org/abs/2210.08566)\n",
    "8. Isabel Nha Minh Le, Oriel Kiss, Julian Schuhmacher, Ivano Tavernelli, Francesco Tacchino, Symmetry-invariant quantum machine learning force fields, arXiv:2311.11362, 2023.\n",
    "9. Oriel Kiss, Francesco Tacchino, Sofia Vallecorsa, Ivano Tavernelli, Quantum neural networks force fields generation, Mach.Learn.: Sci. Technol. 3 035004, 2022.\n",
    "10. Johannes Jakob Meyer, Marian Mularski, Elies Gil-Fuster, Antonio Anna Mele, Francesco Arzani, Alissa Wilms, Jens Eisert, Exploiting Symmetry in Variational Quantum Machine Learning, PRX Quantum 4,010328, 2023.\n",
    "11. David Wierichs, Richard D. P. East, Martn Larocca, M. Cerezo, Nathan Killoran, Symmetric derivatives of parametrized quantum circuits, arXiv:2312.06752, 2023.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
